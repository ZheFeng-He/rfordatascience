---
title: "RA"
author: "Zhefeng He"
date: "2023-09-23"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(palmerpenguins)
library(ggthemes)
palmerpenguins::penguins
```


## Data Visualization Exercise

-1, How many rows are in penguins? How many columns?

```{r, echo = FALSE}
penguins
nrowp = nrow(penguins)
ncolp = ncol(penguins)
```

Penguins has `r nrowp` rows and `r ncolp` columns.

-2, What does the bill_depth_mm variable in the penguins data frame
describe? Read the help for ?penguins to find out.

```{r}
#A: It is a number denoting bill depth (millimeters)
```

-3, Make a scatterplot of bill_depth_mm vs. bill_length_mm. That is,
make a scatterplot with bill_depth_mm on the y-axis and bill_length_mm
on the x-axis. Describe the relationship between these two variables.

```{r, echo = TRUE, warning=FALSE}
ggplot(
  data = penguins,
  mapping = aes(x = bill_length_mm, y = bill_depth_mm)
) +
  geom_point()
```

```{r}
ggplot(
  data = penguins,
  mapping = aes(x = bill_length_mm, y = bill_depth_mm)
) +
  geom_point()
```

```{r}
#A: The scatterplot does not suggest a strong relationship between bill_depth_mm vs. bill_length_mm, as bill_depth_mm data are randomly distributed over bill_length_mm.
```

-4, What happens if you make a scatterplot of species vs. bill_depth_mm?
What might be a better choice of geom?

ggplot( data = penguins, mapping = aes(x = species, y = bill_depth_mm)
) + geom_point()

```{r}
#A: Depth data of each sample is displaced over three categories of species.It might be better to combined species into the previous bill_depth_mm vs. bill_length_mm.
```

-5, Why does the following give an error and how would you fix it?

ggplot(data = penguins) + geom_point()

```{r}
#A: The code did not set the mapping of x-axis and y-axis. I would add "mapping=aes(...)" to include certain variable input and generate the graph.
```

-6, What does the na.rm argument do in geom_point()? What is the default
value of the argument? Create a scatterplot where you successfully use
this argument set to TRUE.

```{r}
#"na.rm" will remove missing values in the target dataset（if coded as "NA").
ggplot(
  data = penguins,
  mapping = aes(x = bill_length_mm, y = bill_depth_mm)
) +
  geom_point(na.rm=TRUE)
```

-7, Add the following caption to the plot you made in the previous
exercise: "Data come from the palmerpenguins package." Hint: Take a look
at the documentation for labs().

```{r}
#Using labs() to include these captions.
ggplot(
  data = penguins,
  mapping = aes(x = bill_length_mm, y = bill_depth_mm)
) + labs( title = "Body mass and flipper length",
    subtitle = "Bill length and depth of Penguins",
    x = "Bill length (mm)", y = "Bill depth(mm)")+
  geom_point(na.rm=TRUE)
```

<<<<<<< HEAD
-8, Recreate the following visualization. What aesthetic should bill_depth_mm be mapped to? And should it be mapped at the global level or at the geom level?

-8, Recreate the following visualization. What aesthetic should
bill_depth_mm be mapped to? And should it be mapped at the global level
or at the geom level?

  
```{r,warning=FALSE, message=FALSE}
ggplot(data = penguins, 
            mapping = aes(x = flipper_length_mm, y = body_mass_g)) + 
       geom_point(aes(color = bill_depth_mm)) + 
       geom_smooth() 

#bill_depth_mm should be mapped to geom_point(aes(color=)), and at the geom level. (BUT WHY?)
```

-9. Run this code in your head and predict what the output will look
like. Then, run the code in R and check your predictions.

ggplot( data = penguins, mapping = aes(x = flipper_length_mm, y =
body_mass_g, color = island) ) + geom_point() + geom_smooth(se = FALSE)

```{r}
#A: It will generate a flipper_lenght_mm vs body_mass_g plot with three different colors on points to identify which island the sample comes from.
```

-10. Will these two graphs look different? Why/why not?

ggplot( data = penguins, mapping = aes(x = flipper_length_mm, y =
body_mass_g) ) + geom_point() + geom_smooth()

ggplot() + geom_point( data = penguins, mapping = aes(x =
flipper_length_mm, y = body_mass_g) ) + geom_smooth( data = penguins,
mapping = aes(x = flipper_length_mm, y = body_mass_g) )

```{r}
#A: Yes, the two graphs will look as same. Because the input of variables are the same for both graphs. They are only different in the way of organizing data (the second one specifies the input in geom(), while the input in those two are the same as mapping.) (Is this a proper answer?)
```

# 2.43 Exercise

-1, Make a bar plot of species of penguins, where you assign species to
the y aesthetic. How is this plot different?

```{r}
#The plot appears to be 'flipped' so the species are appearing on the y axis.
```

-2, How are the following two plots different? Which aesthetic, color or
fill, is more useful for changing the color of bars?

ggplot(penguins, aes(x = species)) + geom_bar(color = "red")

ggplot(penguins, aes(x = species)) + geom_bar(fill = "red")

```{r}
#These two plots are different in the color of bars. The fill is more useful for changing the color of bars
```

-3, What does the bins argument in geom_histogram() do?

```{r}
#Bins stands the number of "buckets" that data is cut into, automatically it is 30.
```

-4, Make a histogram of the carat variable in the diamonds dataset that
is available when you load the tidyverse package. Experiment with
different binwidths. What binwidth reveals the most interesting
patterns?

```{r}
ggplot(diamonds, aes(x = carat)) +
  geom_histogram()
ggplot(diamonds, aes(x = carat)) +
  geom_histogram(bins = 15)

ggplot(diamonds, aes(x = carat)) +
  geom_histogram(binwidth = 1.5)

#The binwidth of 1.5 shows the most interesting pattern with only 3 cuts to the data.
```

# 2.5.5 Exercise

-1, The mpg data frame that is bundled with the ggplot2 package contains
234 observations collected by the US Environmental Protection Agency on
38 car models. Which variables in mpg are categorical? Which variables
are numerical? (Hint: Type ?mpg to read the documentation for the
dataset.) How can you see this information when you run mpg?

```{r}
str(mpg)
#manufacturer, model, year, and class are categorical variables
#displ, cyl, trans, drv, cty, hwy, and fl are numerical variables
#I can use the str(mpg) function, which displays the structure of the dataset, including variable names and data types.
```

-2, Make a scatterplot of hwy vs. displ using the mpg data frame. Next,
map a third, numerical variable to color, then size, then both color and
size, then shape. How do these aesthetics behave differently for
categorical vs. numerical variables?

```{r}
ggplot(mpg, aes(x = displ, y = hwy, color = cyl)) +
  geom_point()
ggplot(mpg, aes(x = displ, y = hwy, size = cyl)) +
  geom_point()

ggplot(mpg, aes(x = displ, y = hwy, color = cyl, size = cyl)) +
  geom_point()
ggplot(mpg, aes(x = displ, y = hwy, shape = drv)) +
  geom_point()

#When numerical variables are mapped to color or size aesthetics, the values of the numerical variable determine the color or size of points. Higher numerical values often correspond to lighter colors or larger sizes. When categorical variables are mapped to color, each category is assigned a unique color. This aids in distinguishing between different categories in the visualization.When categorical variables are mapped to shape, each category is represented by a distinct shape. This approach is valuable for differentiation.
```

-3, In the scatterplot of hwy vs. displ, what happens if you map a third
variable to linewidth?

```{r}
#It would not have effect, since the linewidth aesthetic is typically used for specifying the width of lines in line plots, not for points in scatterplots.
```

-4, What happens if you map the same variable to multiple aesthetics?

```{r}
#It could generate different symbols, size and changes of color depending on the certain condition.
```

<<<<<<< HEAD
-5, Make a scatterplot of bill_depth_mm vs. bill_length_mm and color the points by species. What does adding coloring by species reveal about the relationship between these two variables? What about faceting by species?

```{r,message=FALSE,warning=FALSE}
ggplot(data = penguins,
 mapping = aes(x = bill_length_mm , y = bill_depth_mm))+
  geom_point(aes(color = species))

#Each specie has a distinct cluster that different from other two species in terms of bill_depth_mm vs. bill_length_mm.
```

-6，Why does the following yield two separate legends? How would you fix
it to combine the two legends?

ggplot( data = penguins, mapping = aes( x = bill_length_mm, y =
bill_depth_mm, color = species, shape = species ) ) + geom_point() +
labs(color = "Species")

<<<<<<< HEAD
ggplot(
  data = penguins,
  mapping = aes(
    x = bill_length_mm, y = bill_depth_mm, 
    color = species, shape = species
  )
) +
  geom_point() +
  labs(color = "Species")
  
```{r, warning=FALSE}
#This is because the labs(color = "Species") only generates the legend "Species" in color while missing the shape.
#FIX by combine legends of shape and color 

ggplot(data = penguins,
  mapping = aes(
    x = bill_length_mm, y = bill_depth_mm, 
    color = species, shape = species )) +
  geom_point() +
  scale_color_discrete(name = "Species") +
  scale_shape_discrete(name = "Species")
```

-7, Create the two following stacked bar plots. Which question can you
answer with the first one? Which question can you answer with the second
one?

```{r}
ggplot(penguins, aes(x = island, fill = species)) +
  geom_bar(position = "fill")
ggplot(penguins, aes(x = species, fill = island)) +
  geom_bar(position = "fill")
#First one: what is the distribution of penguin species on different islands?
#Second one: what is the distribution of penguin islands for each species?
```

# 2.6.1 Exercise

-1,Run the following lines of code. Which of the two plots is saved as
mpg-plot.png? Why?

ggplot(mpg, aes(x = class)) + geom_bar() ggplot(mpg, aes(x = cty, y =
hwy)) + geom_point() ggsave("mpg-plot.png")

```{r}
#The second one is saved for it is the latest one.
```

2,What do you need to change in the code above to save the plot as a PDF
instead of a PNG? How could you find out what types of image files would
work in ggsave()?


-To save as a pdf the code needs to be changed from `.png` to `.pdf`at the end of file names.


#3.5 Exercise

-1, Why does this code not work? my_variable \<- 10 my_varıable - \>
Error in eval(expr, envir, enclos): object 'my_varıable' not found

Look carefully! (This may seem like an exercise in pointlessness, but
training your brain to notice even the tiniest difference will pay off
when programming.)

```{r}
#There is a typing error in "my_varıable"
```

-2,Tweak each of the following R commands so that they run correctly:

libary(todyverse)

ggplot(dTA = mpg) + geom_point(maping = aes(x = displ y = hwy)) +
geom_smooth(method = "lm)

```{r}
library(tidyverse)
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
```

-3,Press Option + Shift + K / Alt + Shift + K. What happens? How can you
get to the same place using the menus?

```{r}
#Keyboard Shortcut Quick Reference pops up. Using menus to select the tools -> Keyboard shortcuts help
```

-4, Let's revisit an exercise from the Section 2.6. Run the following
lines of code. Which of the two plots is saved as mpg-plot.png? Why?

my_bar_plot \<- ggplot(mpg, aes(x = class)) + geom_bar() my_scatter_plot
\<- ggplot(mpg, aes(x = cty, y = hwy)) + geom_point() ggsave(filename =
"mpg-plot.png", plot = my_bar_plot)

```{r}
#The first one, since the ggsave selects the specific name of plot.
```

# 4.2.5, Rows Exercise

library(nycflights13)

-1, In a single pipeline for each condition, find all flights that meet
the condition:

```{r}
library(nycflights13) 
#Had an arrival delay of two or more hours
filter(flights, arr_delay >= 120)
#Flew to Houston (IAH or HOU)
filter(flights, dest == "IAH" | dest == "HOU")
#Were operated by United, American, or Delta
filter(flights, carrier %in% c("AA", "DL", "UA"))
#Departed in summer (July, August, and September)
filter(flights, month >= 7, month <= 9)
#Arrived more than two hours late, but didn’t leave late
filter(flights, arr_delay > 120, dep_delay <= 0)
#Were delayed by at least an hour, but made up over 30 minutes in flight
filter(flights, dep_delay >= 60, dep_delay - arr_delay > 30)
```

-2, Sort flights to find the flights with longest departure delays. Find
the flights that left earliest in the morning.

```{r}
arrange(flights, desc(dep_delay))
arrange(flights, dep_delay)
#The most delayed flight was HA 51, JFK to HNL, and Flight B6 97 (JFK to DEN) departed 43 minutes early.
```

-3, Sort flights to find the fastest flights. (Hint: Try including a
math calculation inside of your function.)

```{r}
head(arrange(flights, desc(distance / air_time)))
#DL1499 is the fastest flight in terms of speed.
```

-4,Was there a flight on every day of 2013?

```{r}
flights %>% 
  filter(year == 2013) %>% 
  distinct(month, day)

#YES, every day having a flight.
```

-5,Which flights traveled the farthest distance? Which traveled the
least distance?

```{r}
flights %>% 
  arrange(desc(distance))
flights %>% 
  arrange(distance)
#HA51 travels the farthest distance, and US1632 has the shortest distance.
```

-6,Does it matter what order you used filter() and arrange() if you're
using both? Why/why not? Think about the results and how much work the
functions would have to do.

```{r}
#The order will not affect the result when using both filter() and arrange(), since arrange() only arranging the data instead of filtering it
```

<<<<<<< HEAD

## 4.35 Exercise
=======
# 4.35 Exercise
>>>>>>> 3fa362b4b938c04a07e346096fb3a6c6b8adb433

-1, Compare dep_time, sched_dep_time, and dep_delay. How would you
expect those three numbers to be related?

```{r}
#I will expect: dep_delay = dep_time - sched_dep_time
```

-2, Brainstorm as many ways as possible to select dep_time, dep_delay,
arr_time, and arr_delay from flights.

```{r}
flights %>% 
  select(dep_time, dep_delay, arr_time, arr_delay)
flights %>% 
  select(starts_with("dep"), starts_with("arr"))
flights %>% 
  select(c(dep_time, dep_delay, arr_time, arr_delay))
```

-3, What happens if you specify the name of the same variable multiple
times in a select() call?

```{r}
flights %>% 
  select(dep_time, dep_time, dep_delay, arr_time, arr_delay)
#Nothing happened
```

-4, What does the any_of() function do? Why might it be helpful in
conjunction with this vector?

```{r}
#It returns all the variables you ask for, and this is helpful in finding out certain column in the dataset, as following
variables <- c("year", "month", "day", "dep_delay", "arr_delay")
flights %>% 
  select(any_of(variables))
```

-5, Does the result of running the following code surprise you? How do
the select helpers deal with upper and lower case by default? How can
you change that default?

flights \|\> select(contains("TIME"))

```{r}
#Any column name including TIME is presented in results, which is not accurate. I can change it as following
select(flights, contains("TIME",  ignore.case = FALSE))
```

-6,Rename air_time to air_time_min to indicate units of measurement and
move it to the beginning of the data frame.

```{r}
flights |> 
  rename(air_time_min = air_time)

flights |> 
  relocate(air_time)
```

-7, Why doesn't the following work, and what does the error mean?

flights \|\> select(tailnum) \|\> arrange(arr_delay) \> Error in
`arrange()`: \> ℹ In argument: `..1 = arr_delay`. \> Caused by error: \>
! object 'arr_delay' not found

```{r}
flights |> 
  select(tailnum, arr_delay) |> 
  arrange(arr_delay)
#The select() does not include the arr_delay.
```

<<<<<<< HEAD

## 4.5.7 Exercise.
1, Which carrier has the worst average delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights |> group_by(carrier, dest) |> summarize(n()))
=======
# 4.5.7 Exercise.

1, Which carrier has the worst average delays? Challenge: can you
disentangle the effects of bad airports vs. bad carriers? Why/why not?
(Hint: think about flights \|\> group_by(carrier, dest) \|\>
summarize(n()))

>>>>>>> 3fa362b4b938c04a07e346096fb3a6c6b8adb433
```{r}
flights %>%
  group_by(carrier) %>%
  summarise(arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
  arrange(desc(arr_delay))
#F9 has the worst average delays (F9 is Frontier Airline) 
```

-2, Find the flights that are most delayed upon departure from each
destination.

-3, How do delays vary over the course of the day. Illustrate your
answer with a plot.

-4, What happens if you supply a negative n to slice_min() and friends?

-5, Explain what count() does in terms of the dplyr verbs you just
learned. What does the sort argument to count() do?

-6, Suppose we have the following tiny data frame:

df \<- tibble( x = 1:5, y = c("a", "b", "a", "a", "b"), z = c("K", "K",
"L", "L", "K") )

Write down what you think the output will look like, then check if you
were correct, and describe what group_by() does.

df \|\> group_by(y)

Write down what you think the output will look like, then check if you
were correct, and describe what arrange() does. Also comment on how it's
different from the group_by() in part (a)?

df \|\> arrange(y)

Write down what you think the output will look like, then check if you
were correct, and describe what the pipeline does.

df \|\> group_by(y) \|\> summarize(mean_x = mean(x))

Write down what you think the output will look like, then check if you
were correct, and describe what the pipeline does. Then, comment on what
the message says.

df \|\> group_by(y, z) \|\> summarize(mean_x = mean(x))

Write down what you think the output will look like, then check if you
were correct, and describe what the pipeline does. How is the output
different from the one in part (d).

df \|\> group_by(y, z) \|\> summarize(mean_x = mean(x), .groups =
"drop")

Write down what you think the outputs will look like, then check if you
were correct, and describe what each pipeline does. How are the outputs
of the two pipelines different?

df \|\> group_by(y, z) \|\> summarize(mean_x = mean(x))

df \|\> group_by(y, z) \|\> mutate(mean_x = mean(x))

# 5.6 Exercise

-1,Restyle the following pipelines following the guidelines above.

flights\|\>filter(dest=="IAH")\|\>group_by(year,month,day)\|\>summarize(n=n(),
delay=mean(arr_delay,na.rm=TRUE))\|\>filter(n\>10)

flights\|\>filter(carrier=="UA",dest%in%c("IAH","HOU"),sched_dep_time\>
0900,sched_arr_time\<2000)\|\>group_by(flight)\|\>summarize(delay=mean(
arr_delay,na.rm=TRUE),cancelled=sum(is.na(arr_delay)),n=n())\|\>filter(n\>10)

```{r}
#Repair
flights|>
  filter(dest=="IAH")|>
  group_by(year,month,day)|>
  summarize(n=n(),
delay=mean(arr_delay,na.rm=TRUE))|>
  filter(n>10)

flights|>
  filter(carrier=="UA",dest%in%c("IAH","HOU"),sched_dep_time>
0900,sched_arr_time<2000)|>
  group_by(flight)|>
  summarize(delay=mean(
arr_delay,na.rm=TRUE),cancelled=sum(is.na(arr_delay)),n=n()
)|>
  filter(n>10)
```

# 6.2.1

-1, For each of the sample tables, describe what each observation and
each column represents.

```{r}
#Table 1 includes columns: country, year, cases, population with each observation on a row and value of a variable for the observation.
#Table 2 has columns: country, year, type, count. Type is a Character variable to indicate if a country is cases or population.
#Table 3 has columns: country, year, and rate. The rate is an emerged column of cases and population from table 1.
#Table 4 is split across two tables. The first one gives the cases with the column variables being years, while the second one gives population instead of cases.
```

-2, Sketch out the process you'd use to calculate the rate for table2
and table3. You will need to perform four operations:

Extract the number of TB cases per country per year. Extract the
matching population per country per year. Divide cases by population,
and multiply by 10000. Store back in the appropriate place. You haven't
yet learned all the functions you'd need to actually perform these
operations, but you should still be able to think through the
transformations you'd need.

```{r}
#Extract the number of TB cases per country per year.
table2_cases <- table2 %>%
  filter(type == "cases")

#Extract the matching population per country per year
table2_pop <- table2 %>%
    filter(type == "population")

#Divide cases by population, and multiply by 10000 
table2_com <- tibble(
  country = table2_cases$country,
  year = table2_cases$year,
  cases = table2_cases$count,
  population = table2_pop$count
  )
#Store back in the appropriate place.
table2_com <- table2_com %>%
    mutate(rate = (cases / population) * 10000)
```

-Notes for new verbs: pivot_longer(), from row to
column/parse_number()/names_sep =, delete certain character/distinct
()/pivot_wider(), from column to row/names_from = "some
column/row"/id_cols = starts_with ("select specific values")

-Difference between tibble() and tribble ()?

# 7.3 Exercise.

-1, Go to the RStudio Tips Twitter account,
<https://twitter.com/rstudiotips> and find one tip that looks
interesting. Practice using it!

-2, What other common mistakes will RStudio diagnostics report? Read
<https://support.posit.co/hc/en-us/articles/205753617-Code-Diagnostics>
to find out.

# 8.2.4

-What function would you use to read a file where fields were separated
with "\|"?

```{r}
#We could use "read_delim" to read files containing delimiter
```

-Apart from file, skip, and comment, what other arguments do read_csv()
and read_tsv() have in common?

```{r}
#Common: col_names, col_types, col_select, id, locale, na, trim_ws, quoted_na, quote, comment,  skip, n_max, guess_max, progress, name_repair, num_threads, show_col_types, skip_empty_rows, lazy
```

-What are the most important arguments to read_fwf()?

```{r}
#I would say "fwf_width" and "fwf_positions" are most important, as they specify fields which defines vectors for extract data.
```

-Sometimes strings in a CSV file contain commas. To prevent them from
causing problems, they need to be surrounded by a quoting character,
like " or '. By default, read_csv() assumes that the quoting character
will be ". To read the following text into a data frame, what argument
to read_csv() do you need to specify?

"x,y\n1,'a,b'"

```{r}
#Probably use "quoted_na = TRUE" or "quote = "
read_csv("x,y\n1,'a,b'", quote = "'")
```

-Identify what is wrong with each of the following inline CSV files.
What happens when you run the code?

read_csv("a,b\n1,2,3\n4,5,6") read_csv("a,b,c\n1,2\n1,2,3,4")
read_csv("a,b\n"1") read_csv("a,b\n1,2\na,b") read_csv("a;b\n1;3")

```{r}
read_csv("a,b\n1,2,3\n4,5,6")
#Prasing issues in column specification. Second row has two values, while the third row has three values.

read_csv("a,b,c\n1,2\n1,2,3,4")
#There are three header columns in the data frame, while the following row includes four values, which does not match the number of colums.

read_csv("a,b\n\"1")
#The dataset includes two header columns but only specifies one value in the firts row.

read_csv("a,b\n1,2\na,b")

read_csv("a;b\n1;3")
#For dataset includes ";", should use read_csv2()
```

-Practice referring to non-syntactic names in the following data frame
by:

Extracting the variable called 1. Plotting a scatterplot of 1 vs. 2.
Creating a new column called 3, which is 2 divided by 1. Renaming the
columns to one, two, and three. annoying \<- tibble( `1` = 1:10, `2` =
`1` \* 2 + rnorm(length(`1`)) )

```{r}
annoying <- tibble(
  `1` = 1:10,
  `2` = `1` * 2 + rnorm(length(`1`))
)

annoying %>%
  select('1' = 1, '2' )


```



# 9 Workflow: getting help

## 9.1 Google is your friend

- It is helpful to use Google and Stack Overflow to solve code errors.

## 9.2 Making a reprex

- Make code reproducible by reprex() to format default output in github. Use dput() to generate the R code needed to recreate it

## 9.3 Investing in yourself

 1, doing tidyverse blog (https://www.tidyverse.org/blog/). 2, reading R_Weekly (https://rweekly.org/)

# 10 Layers

## 10.1 Introduction
- library(tidyverse)

## 10.2 Aesthetic mappings
- ggplot2 only use six shapes at a time. 

- Mapping an unordered discrete (categorical) variable (class) to an ordered aesthetic (size or alpha) is generally not a good idea because it implies a ranking that does not in fact exist. 

- You can customize size, shape, and color by inputing different numbers (for size and shape) and color names (for color).

### 10.2.1 Exercises

 1, Create a scatterplot of hwy vs. displ where the points are pink filled in triangles.

```{r}
library(tidyverse)
ggplot(mpg, aes(x = hwy, y = displ)) + 
  geom_point(color = "pink", shape = 17)
```
 2. Why did the following code not result in a plot with blue points?

- A: The code failed to add aesthetic mapping to the "color ="

 3. What does the stroke aesthetic do? What shapes does it work with? (Hint: use ?geom_point)

- A: Stroke modifies the width of the border to assist coloring the inside and outside differently. It works with shapes having a border.

 4. What happens if you map an aesthetic to something other than a variable name, like aes(color = displ < 5)? Note, you’ll also need to specify x and y.
```{r}
ggplot(mpg, aes(x = hwy, y = displ,color = displ < 5))+geom_point()
```

- A: It generates a logical variable and default set different colors for points with dipsl <5 and >= 5.

## 10.3 Geometric objects

-geom_smooth() separates the graph into various lines based on a category variable. Other types of geom including geom_histogram(), geom_density(), geom_boxplot(), and more (https://ggplot2.tidyverse.org/reference.) .

### 10.3.1 Exercises
 1. What geom would you use to draw a line chart? A boxplot? A histogram? An area chart?
- A: Shouldn't we use geom_line() for a line chart? None of the three mentioned in question.

 2. Earlier in this chapter we used show.legend without explaining it:

ggplot(mpg, aes(x = displ, y = hwy)) +
  geom_smooth(aes(color = drv))

What does show.legend = FALSE do here? What happens if you remove it? Why do you think we used it earlier?

- A: "show.legend = FALSE" will remove the notification of dry with each color in the right. Removing it will have the color notification pops up in the right. I think we used it earlier for clarifying which points of different color belongs to different drv.

 3. What does the se argument to geom_smooth() do?
- A: Display confidence interval around smooth? (TRUE by default, see level to control.)

 4. Recreate the R code necessary to generate the following graphs. Note that wherever a categorical variable is used in the plot, it’s drv.
```{r}
ggplot(mpg, aes(x = displ, y = hwy)) +
  geom_point() +
  geom_smooth(se = FALSE)

ggplot(mpg, aes(x = displ, y = hwy)) +
  geom_smooth(mapping = aes(group = drv), se = FALSE) +
  geom_point()

ggplot(mpg, aes(x = displ, y = hwy, colour = drv)) +
  geom_point() +
  geom_smooth(se = FALSE)

ggplot(mpg, aes(x = displ, y = hwy)) +
  geom_point(aes(colour = drv)) +
  geom_smooth(se = FALSE)

ggplot(mpg, aes(x = displ, y = hwy, colour = drv, linetype = drv)) +
  geom_point() +
  geom_smooth( se = FALSE)

ggplot(mpg, aes(x = displ, y = hwy, colour = drv)) +
  geom_point(size = 4, color = "white") +
  geom_point()
```

## 10.4 Facets

- facet_wrap() splits a plot into subplots. Also facet_grid(), which is a double sided formula: rows~cols. We add scales to the facet_grid().

### 10.4.1 Exercies
 1 What happens if you facet on a continuous variable?
- It will return multiple subplots. Each subplot contains a certain value of the continuous variable.

 2. What do the empty cells in the plot above with facet_grid(drv ~ cyl) mean? Run the following code. How do they relate to the resulting plot?
```{r}
ggplot(mpg) + 
  geom_point(aes(x = drv, y = cyl))
```
- Empty cells in the plot above stands for no observations for the formula drv ~ cyl in the data set. The plot generated by codes in question plot the combination of drv and cyl. These points are not displayed in the previous plot.

 3.What plots does the following code make? What does . do?
```{R}
ggplot(mpg) + 
  geom_point(aes(x = displ, y = hwy)) +
  facet_grid(drv ~ .)

ggplot(mpg) + 
  geom_point(aes(x = displ, y = hwy)) +
  facet_grid(. ~ cyl)
```
- The ',' removes the dimension of facet_grid(). The first plot contains values of drv on y-axis, while the second places values of cyl on x-axis.

 4.Take the first faceted plot in this section:
```{r}
ggplot(mpg) + 
  geom_point(aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)
```
  What are the advantages to using faceting instead of the color aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset?
- Advantage: Specify each categories of variables. Disadvantage: Lack of color for clear identification?

 5.Read ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol arguments?
- nrow and nocl determines the number of rows and columns correspondingly in facet_wrap(). They are not used in facet_grid(), as facet_grid() does not require specifying number of rows and columns. 

 6. Which of the following plots makes it easier to compare engine size (displ) across cars with different drive trains? What does this say about when to place a faceting variable across rows or columns?
```{r, message=FALSE}
ggplot(mpg, aes(x = displ)) + 
  geom_histogram() + 
  facet_grid(drv ~ .)

ggplot(mpg, aes(x = displ)) + 
  geom_histogram() +
  facet_grid(. ~ drv)
```

 Recreate the following plot using facet_wrap() instead of facet_grid(). How do the positions of the facet labels change?
```{r}
ggplot(mpg) + 
  geom_point(aes(x = displ, y = hwy)) +
  facet_grid(drv ~ .)

ggplot(mpg) + 
  geom_point(aes(x = displ, y = hwy)) +
  facet_wrap(drv ~ .)
```

- Place values of drv on x-axis make it easier compare engine size (displ) across cars with different drive trains. It is better align the comparison at the same axis for quick comparison.
- facet_wrap() changes the facet labels to the x-axis.

## 10.5 Statistical transformations

- geom_bar(stat = "identity") to map the height of the bars to the raw values of a y variable. stat_summary() can summarize  the y values for each unique x value. And always use ?stat_bin for help.

### 10.5.1 Exercies

 1.What is the default geom associated with stat_summary()? How could you rewrite the previous plot to use that geom function instead of the stat function?
```{r}
ggplot(data = diamonds) +
  geom_pointrange(
    mapping = aes(x = cut, y = depth),
    stat = "summary",
    fun.min = min,
    fun.max = max,
    fun = median)
```
- A: The default geom asociated with stat_summary() is geom_pointrange()

 2.What does geom_col() do? How is it different from geom_bar()?
-A:geom_col() makes the heights of the bars to represent values in the data. It is different from geom_bar in the function of bars and default.


 3.Most geoms and stats come in pairs that are almost always used in concert. Make a list of all the pairs. What do they have in common? (Hint: Read through the documentation.)
- A: Google for the table. Many geoms and stats have stat_identity as the default sta.

 4.What variables does stat_smooth() compute? What arguments control its behavior?
- A: It computes y, ymax, and ymin. The argument "method", "formula", "method.arg" controls its behavior.

 5.In our proportion bar chart, we need to set group = 1. Why? In other words, what is the problem with these two graphs?
```{r, warning=FALSE, message=FALSE}
ggplot(diamonds, aes(x = cut, y = after_stat(prop))) + 
  geom_bar()
ggplot(diamonds, aes(x = cut, y = after_stat(prop), group=1)) + 
  geom_bar()
ggplot(diamonds, aes(x = cut, fill = color, y = after_stat(prop))) + 
  geom_bar()

ggplot(diamonds, aes(x = cut, fill = color, y = after_stat(prop), group=1)) +
  geom_bar()
```
- Without group = 1, the plot does not represent the proportion of each group in the dataset. Besides, when use group = 1 with fill, we need to take care of the grouping structure.

## 10.6 Position adjustments
- With a set x, whatever variables put in fill would return the combination of the variable with the set X.
- Three other opotions "identity", "dodge", and "fill"
- position = "identity" will place each object exactly where it falls in the context of the graph. This is not very useful for bars, because it overlaps them.
- Position = "fill" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups.
- Position = "dodge" places overlapping objects directly beside one another. This makes it easier to compare individual values.
- Position = "jitter" adds a small amount of random noise to each point.

### 10.6.1
 1.What is the problem with the following plot? How could you improve it?
```{r}
ggplot(mpg, aes(x = cty, y = hwy)) + 
  geom_point()
ggplot(mpg, aes(x = cty, y = hwy)) + 
  geom_point(position = "jitter")
```

- The combination of cty and hwy include lots of overlapping.We can add a position = "jitter"to create noise for each point.

 2.What, if anything, is the difference between the two plots? Why?
```{r}
ggplot(mpg, aes(x = displ, y = hwy)) +
  geom_point()
ggplot(mpg, aes(x = displ, y = hwy)) +
  geom_point(position = "identity")
```
- There is no graphical difference between two plots, since position = "identity" will place each object exactly where it falls in the context of the graph, as default geom_point() represents.

 3. What parameters to geom_jitter() control the amount of jittering?
-The width and height.

 4. Compare and contrast geom_jitter() with geom_count().
-geom_count would not change the poisition of points, but would overlap point with close range to eact other, while geom_jitter() would add random variations to points.

 5. What’s the default position adjustment for geom_boxplot()? Create a visualization of the mpg dataset that demonstrates it.
```{r}
ggplot(data = mpg, aes(x = hwy, y = displ, colour = class)) +
  geom_boxplot(position = "identity")
```
- The default position adjustment is position_dodge2.

## 10.7 Coordinate systems
- coord_quickmap() sets the aspect ratio correctly for geographic maps. coord_polar() uses polar coordinates.

### 10.7.1 Exericises
 1.Turn a stacked bar chart into a pie chart using coord_polar().
```{r}
ggplot(mpg, aes(x = factor(1), fill = class)) +
  geom_bar()

ggplot(mpg, aes(x = factor(1), fill = class)) +
  geom_bar(width = 1) +
  coord_polar(theta = "y")
```

 2.What’s the difference between coord_quickmap() and coord_map()?
-? coord_quickmap(). ?coord_map()
- coord_quickmap() makes maps faster but more approxmiate than coord_map()

 3.What does the following plot tell you about the relationship between city and highway mpg? Why is coord_fixed() important? What does geom_abline() do?
```{r}
ggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +
  geom_point() + 
  geom_abline() +
  coord_fixed()
```

- Without coord_fixed(), the coordinate would change and miss some points. geom_abline() plays a role of adding the fixed makes it easy to compare the highway and city mileage.

## 10.8 The layered grammar of graphics
- Select the necessary data from raw data to create the graph you needed.

# 11 Exploratory data analysis

## 11.1 Introduction
- Questions around the data/Search for answers by visualizing, transforming, and modelling your data/Use what you learn to refine your questions and/or generate new questions.

## 11.2 Questions
- Using visualization and grouping. How are the observations within each subgroup similar to each other? How are the observations in separate clusters different from each other? How can you explain or describe the clusters? Why might the appearance of clusters be misleading?

- coord_cartesian() can be used to select unusual values

## 11.3.3 Exercises
 1.Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth.
```{r}
#Distribution of x
ggplot(diamonds) +
  geom_histogram(mapping = aes(x = x), binwidth = 0.01)

#Distribution of y
ggplot(diamonds) +
  geom_histogram(mapping = aes(x = y), binwidth = 0.01)

#Distribution of z
ggplot(diamonds) +
  geom_histogram(mapping = aes(x = z), binwidth = 0.01)
```
- All three variables are right skewed in general and having some noticing outliners. Probably y is length for its larger values in general, z would be the width, and x is the depth.

 2.Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the binwidth and make sure you try a wide range of values.)
```{r}
ggplot(diamonds, aes(x = price)) +
  geom_histogram(binwidth = 10, center = 0)
```
- The price is left skewed in general, and has a lot of outlines.Most points are located below 2000$.

 3.How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference?
```{r}
#Diamonds with 0.99 and 1 carat
diamonds %>%
  filter(carat >= 0.99, carat <= 1) %>%
  count(carat)
```
- There are 1558 1-carat diamonds and 23 0.99-carat diamonds. The difference could be natural pheromone that 0.99-carat diamonds are naturally more rare.

 4.Compare and contrast coord_cartesian() vs. xlim() or ylim() when zooming in on a histogram. What happens if you leave binwidth unset? What happens if you try and zoom so only half a bar shows?
```{r, warning=FALSE, message=FALSE}
#Leave bin width unset
ggplot(diamonds) +
  geom_histogram(mapping = aes(x = price)) +
  coord_cartesian(xlim = c(114, 514), ylim = c(1000, 4000))

ggplot(diamonds) +
  geom_histogram(mapping = aes(x = price)) +
  xlim(114, 514) +
  ylim(1000, 4000)
```
- coord_cartesian() is zooming a particular part of the entire coordinate, while xlim() and ylim() select the particular part out of the coordinate. As we see above, the xlim() and ylim() cut off points out of the range selected. Leave bin width unset has no real effect on the display.

## 11.4 Unusual Vlaves & Exercises
- Drop rows with strange values.

 1.What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference in how missing values are handled in histograms and bar charts?

- (Google)The missing values in a histogram would be removed. "In the geom_bar() function, NA is treated as another category. The x aesthetic in geom_bar() requires a discrete (categorical) variable, and missing values act like another category. In a histogram, the x aesthetic variable needs to be numeric, and stat_bin() groups the observations by ranges into bins. Since the numeric value of the NA observations is unknown, they cannot be placed in a particular bin, and are dropped."

 2.What does na.rm = TRUE do in mean() and sum()?

- It reomves NA values before calculating mean and sum.

 3.Recreate the frequency plot of scheduled_dep_time colored by whether the flight was cancelled or not. Also facet by the cancelled variable. Experiment with different values of the scales variable in the faceting function to mitigate the effect of more non-cancelled flights than cancelled flights.
```{r}
nycflights13::flights |> 
  mutate(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + (sched_min / 60)
  ) |> 
  ggplot(aes(x = sched_dep_time)) + 
  geom_freqpoly(aes(color = cancelled), binwidth = 1/4)
```

## 11.5
- Covariation is the tendency for the values of two or more variables to vary together in a related way. 

### 11.5.1 A categorical and a numerical variable
- fct_reorder() to reorder variables for a more information disply.
```{r}
ggplot(mpg, aes(x = fct_reorder(class, hwy, median), y = hwy)) +
  geom_boxplot()
```

### 11.5.1.1 Exercises

 1.Use what you’ve learned to improve the visualization of the departure times of cancelled vs. non-cancelled flights.
```{r, warning=FALSE}
nycflights13::flights |> 
  mutate(cancelled = is.na(dep_time) | is.na(arr_time)) %>% 
  ggplot() +
  geom_boxplot(aes(x = cancelled, y = dep_time))
```

 2.Based on EDA, what variable in the diamonds dataset appears to be most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?
```{r}
#Correlation between carat and price
ggplot(diamonds) +
  geom_point(aes(x = carat, y = price), color = "green", alpha = 0.76)

#Correlation between depth and price
ggplot(diamonds) +
  geom_point(aes(x = depth, y = price), color = "green", alpha = 0.76)

#Correlation between z and price
ggplot(diamonds) +
  geom_point(aes(x = z, y = price), color = "green", alpha = 0.76)

#Correlation between x and price
ggplot(diamonds) +
  geom_point(aes(x = x, y = price), color = "green", alpha = 0.76)
```
- It appears that x has the highest correlation with price among these variables.

 3.Instead of exchanging the x and y variables, add coord_flip() as a new layer to the vertical boxplot to create a horizontal one. How does this compare to exchanging the variables?

 4.One problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot. Install the lvplot package, and try using geom_lv() to display the distribution of price vs. cut. What do you learn? How do you interpret the plots?

 5.Create a visualization of diamond prices vs. a categorical variable from the diamonds dataset using geom_violin(), then a faceted geom_histogram(), then a colored geom_freqpoly(), and then a colored geom_density(). Compare and contrast the four plots. What are the pros and cons of each method of visualizing the distribution of a numerical variable based on the levels of a categorical variable?

 6.If you have a small dataset, it’s sometimes useful to use geom_jitter() to avoid overplotting to more easily see the relationship between a continuous and categorical variable. The ggbeeswarm package provides a number of methods similar to geom_jitter(). List them and briefly describe what each one does.

### 11.5.2 Two categorical variables
- To create the plot with two categorical variables, use geom_count()
- Use dplyr to computing the counts between these variables.
- Visualize with geom_tile() and the fill aesthetic.

### 11.5.3 Two numerical variables
- Surely use geom_point() to plot the numerical variables.alpha aesthetic can add transparency.
- New tools to bin in one dimension: geom_bin2d() and geom_hex().

### 11.6 Patterns and models
- For a systematic relationship exists between two variables, it will appear as a pattern in the data. Such pattern should be considered: if it is a coincidence? What relationship implied by the pattern? How strong the relationship is? What other verbs may affect the relationship? Does the relationship change if you look at individual subgroups of the data?

# 12  Communication
 
## 12.1.1 Prerequisites
```{r}
library(tidyverse)
library(scales)
library(ggrepel)
library(patchwork)
```

## 12.2 Labels
- labs() adds names to elements in the coordinate. Those elements include x, y, color, title, subtitle, caption.

### 12.2.1 Exercises

 1.Create one plot on the fuel economy data with customized title, subtitle, caption, x, y, and color labels.
```{r}
  ggplot()+
  geom_point(data = mpg, aes( x = hwy, y = displ, colour = drv, shape = drv))+
  labs( x = "Engine displacement (L)",
        y = "Highway fuel economy (mpg)",
        title = "Large engine displacement results in lower gas mileage performance",
        subtitle = "SUV and pickup classes have more small engine & high mpg combination",
        caption = "Dataset from tidyverse")
```

 2.Recreate the following plot using the fuel economy data. Note that both the colors and shapes of points vary by type of drive train.
```{r}
  ggplot(mpg, aes(x = cty, y= hwy, shape = drv, color = drv))+
  geom_point()+
  labs( x = "City MPG",
        y = "Highway MPG",
        shape = "Type of drive train")
```
 3.Take an exploratory graphic that you’ve created in the last month, and add informative titles to make it easier for others to understand.
 
# 12.3 Annotations

## 12.3.1 Exercises

1.Use geom_text() with infinite positions to place text at the four corners of the plot.
```{r}
label <- tribble(
  ~displ, ~hwy, ~label, ~vjust, ~hjust,
  Inf, Inf, "Top right", "top", "right",
  Inf, -Inf, "Bottom right", "bottom", "right",
  -Inf, Inf, "Top left", "top", "left",
  -Inf, -Inf, "Bottom left", "bottom", "left"
)

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label, vjust = vjust, hjust = hjust), data = label)
```

2. Use annotate() to add a point geom in the middle of your last plot without having to create a tibble. Customize the shape, size, or color of the point.
```{r}
ggplot(mpg, aes(x = displ, y = hwy)) +
  geom_point() +
   annotate(geom = "label", x= max(mpg$displ), y= max(mpg$hwy),
    label = "Top right", vjust = "top",
    hjust = "right", color = "red"
  ) +
   annotate(geom = "label", x= min(mpg$displ), y= max(mpg$hwy),
    label = "Top left", vjust = "top",
    hjust = "left", color = "red"
  ) +
   annotate(geom = "label", x= max(mpg$displ), y= min(mpg$hwy),
    label = "Bottom right", vjust = "bottom",
    hjust = "right", color = "red"
  ) +
   annotate(geom = "label", x= min(mpg$displ), y= min(mpg$hwy),
    label = "Bottom left", vjust = "bottom",
    hjust = "left", color = "red"
  ) 
```

3. How do labels with geom_text() interact with faceting? How can you add a label to a single facet? How can you put a different label in each facet? (Hint: Think about the dataset that is being passed to geom_text().)
```{r}

# labels in each different plots
label <- tibble(
  displ = Inf,
  hwy = Inf,
  class = unique(mpg$class),
  label = str_c("Label for ", class)
)

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label),
    data = label, vjust = "top", hjust = "right",
    size = 3
  ) +
  facet_wrap(~class)
```


4. What arguments to geom_label() control the appearance of the background box?

-label.padding: padding around label
-label.r: amount of rounding in the corners
-label.size: size of label border

5. What are the four arguments to arrow()? How do they work? Create a series of plots that demonstrate the most important options.
 
-angle : angle of arrow head
-length : length of the arrow head
-ends: ends of the line to draw arrow head
-type: "open" or "close": whether the arrow head is a closed or open triangle

## 12.4 Scales

### 12.4.1 Default scales
- scale_ followed by the name of the aesthetic, then _, then the name of the scale. The default scales are named according to the type of variable they align with: continuous, discrete, datetime, or date. scale_x_continuous() puts the numeric values from displ on a continuous number line on the x-axis, scale_color_discrete() chooses colors for each of the class of car, etc.

### 12.4.2 Axis ticks and legend keys

-There are two primary arguments that affect the appearance of the ticks on the axes and the keys on the legend: breaks and labels. Breaks controls the position of the ticks, or the values associated with the keys. Labels controls the text label associated with each tick/key.

-We can use the labels in the same way. label_dollar will add dollar sign. label_percent() add percentage.

### 12.4.3 Legend layout

-To control the overall position of the legend, you need to use a theme() setting.The theme setting legend.position controls where the legend is drawn.

-To control the display of individual legends, use guides() along with guide_legend() or guide_colorbar(). Note that the name of the argument in guides() matches the name of the aesthetic, just like in labs().

### 12.4.4 Replacing a scale

-It’s very useful to plot transformations of your variable. The ColorBrewer scales are documented online at https://colorbrewer2.org/ and made available in R via the RColorBrewer package, by Erich Neuwirth. 

-For continuous color, you can use the built-in scale_color_gradient() or scale_fill_gradient(). If you have a diverging scale, you can use scale_color_gradient2().

-Note that all color scales come in two varieties: scale_color_*() and scale_fill_*() for the color and fill aesthetics respectively (the color scales are available in both UK and US spellings).

### 12.4.5 Zooming

-There are three ways to control the plot limits:

-Adjusting what data are plotted.
-Setting the limits in each scale.
-Setting xlim and ylim in coord_cartesian().

-To zoom in on a region of the plot, it’s generally best to use coord_cartesian().

-Setting the limits on individual scales is generally more useful if you want to expand the limits, e.g., to match scales across different plots.

### 12.4.6 Exercises

1. Why doesn’t the following code override the default scale?
```{r}
df <- tibble(
  x = rnorm(10000),
  y = rnorm(10000)
)

ggplot(df, aes(x, y)) +
  geom_hex() +
  scale_color_gradient(low = "white", high = "red") +
  coord_fixed()
```
-Because the colors in geom_hex() are set by the fill aesthetic, not the color aesthetic.

2. What is the first argument to every scale? How does it compare to labs()?

-The first argument to every scale is the label for the scale. It is equivalent to using the labs function.


3. Change the display of the presidential terms by:

a.Combining the two variants that customize colors and x axis breaks.
b.Improving the display of the y axis.
c.Labelling each term with the name of the president.
d.Adding informative plot labels.
e.Placing breaks every 4 years (this is trickier than it seems!).
```{r}
fouryears <- lubridate::make_date(seq(year(min(presidential$start)),
  year(max(presidential$end)),
  by = 4
), 1, 1)

presidential %>%
  mutate(
    id = 33 + row_number(),
    name_id = fct_inorder(str_c(name, " (", id, ")"))
  ) %>%
  ggplot(aes(start, name_id, colour = party)) +
  geom_point() +
  geom_segment(aes(xend = end, yend = name_id)) +
  scale_colour_manual("Party", values = c(Republican = "red", Democratic = "blue")) +
  scale_y_discrete(NULL) +
  scale_x_date(NULL,
    breaks = presidential$start, date_labels = "'%y",
    minor_breaks = fouryears
  ) +
  ggtitle("Terms of US Presdients",
    subtitle = "Roosevelth (34th) to Obama (44th)"
  ) +
  theme(
    panel.grid.minor = element_blank(),
    axis.ticks.y = element_blank()
  )
```
4.First, create the following plot. Then, modify the code using override.aes to make the legend easier to see.
```{r}
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_point(aes(color = cut), alpha = 1/20)+
  theme(legend.position = "bottom")+
  guides(color = guide_legend(nrow=2, override.aes = list(alpha = 1)))
```


## 12.5 Themes

### 12.5.1 Exercises
1. Pick a theme offered by the ggthemes package and apply it to the last plot you made.

```{r}
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_point(aes(color = cut), alpha = 1/20)+
  theme(legend.position = c(0.6, 0.7),
    legend.direction = "horizontal",
    legend.box.background = element_rect(color = "black"),
    plot.title = element_text(face = "bold"),
    plot.title.position = "plot",
    plot.caption.position = "plot",
    plot.caption = element_text(hjust = 0))+
  guides(color = guide_legend(nrow=2, override.aes = list(alpha = 1)))
```

## 12.6 Layout

### 12.6.1 Exercises
1. What happens if you omit the parentheses in the following plot layout. Can you explain why this happens?
```{r}
p1 <- ggplot(mpg, aes(x = displ, y = hwy)) + 
  geom_point() + 
  labs(title = "Plot 1")
p2 <- ggplot(mpg, aes(x = drv, y = hwy)) + 
  geom_boxplot() + 
  labs(title = "Plot 2")
p3 <- ggplot(mpg, aes(x = cty, y = hwy)) + 
  geom_point() + 
  labs(title = "Plot 3")
(p1 | p2) / p3

```

-The plot layout would not be generated. Because，the parentheses designs the position of different plots in the layout.

2. Using the three plots from the previous exercise, recreate the following patchwork.

Three plots: Plot 1 is a scatterplot of highway mileage versus engine size. Plot 2 is side-by-side box plots of highway mileage versus drive train. Plot 3 is side-by-side box plots of city mileage versus drive train. Plots 1 is on the first row. Plots 2 and 3 are on the next row, each span half the width of Plot 1. Plot 1 is labelled "Fig. A", Plot 2 is labelled "Fig. B", and Plot 3 is labelled "Fig. C".

```{r}
po1 <- ggplot(mpg, aes(x = displ, y = hwy)) + 
  geom_point() + 
  labs(title = "Plot 1")

po2 <- ggplot(mpg, aes(x = drv, y = hwy)) + 
  geom_boxplot() + 
  labs(title = "Plot 2")

po3 <- ggplot(mpg, aes(x = cty, y = hwy)) + 
  geom_point() + 
  labs(title = "Plot 3")
po1/(po2 | po3)
```


# 13  Logical vectors

## 13.1 Introduction

### 13.1.1 Prerequisites
```{r}
library(tidyverse)
library(nycflights13)
#do to a variable inside a data frame with mutate()
```

## 13.2 Comparison
-We use filter() and <, <=, >, >=, != to make Comparison.
-Use digits to compare floating point


1. How does dplyr::near() work? Type near to see the source code. Is sqrt(2)^2 near 2?
```{r}
near(sqrt(2)^2, 2)
```

2. Use mutate(), is.na(), and count() together to describe how the missing values in dep_time, sched_dep_time and dep_delay are connected.

```{r}
flights |> 
    mutate(dep_time_na = is.na(dep_time),
         sched_dep_time_na = is.na(sched_dep_time),
         dep_delay_na = is.na(dep_delay)) |>
  count(dep_time_na, sched_dep_time_na, dep_delay_na)
```


## 13.3 Boolean algebra

1.Find all flights where arr_delay is missing but dep_delay is not. Find all flights where neither arr_time nor sched_arr_time are missing, but arr_delay is.
```{r}
#Find all flights where arr_delay is missing but dep_delay is not.
flights |>
  filter(is.na(arr_delay & ! dep_delay))

#Find all flights where neither arr_time nor sched_arr_time are missing
nycflights13::flights |>
  filter(is.na(sched_arr_time & arr_time & !arr_delay))
```


2.How many flights have a missing dep_time? What other variables are missing in these rows? What might these rows represent?
```{r}
#How many flights have a missing dep_time
flights |>
  count(is.na(dep_time))
#What other variables are missing in these rows?
summary(flights)
```

-These rows might represent the cancelled flight

3.Assuming that a missing dep_time implies that a flight is cancelled, look at the number of cancelled flights per day. Is there a pattern? Is there a connection between the proportion of cancelled flights and the average delay of non-cancelled flights?

```{r, message=FALSE}
#definitely cancelled.
cancelled_per_day <-
  flights %>%
  mutate(cancelled = (is.na(arr_delay) | is.na(dep_delay))) %>%
  group_by(year, month, day) %>%
  summarise(
    cancelled_num = sum(cancelled),
    flights_num = n(),
  )
# It is likely that days with more flights would have a higher probability of cancellations

ggplot(cancelled_per_day) +
  geom_point(aes(x = flights_num, y = cancelled_num))

#Is there a connection between the proportion of cancelled flights and the average delay of non-cancelled flights?

flights %>% group_by(month, day) %>%
  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE),
            prop_cancelled = sum(is.na(dep_time)/n())) %>%
  ggplot(mapping = aes(x = avg_dep_delay, y = prop_cancelled)) +
  geom_point() +
  geom_smooth(method = 'lm', se = FALSE)
```


## 13.4 Summaries

### 13.4.1 Logical summaries

-There are two main logical summaries: any() and all(). any(x) is the equivalent of |; it’ll return TRUE if there are any TRUE’s in x. all(x) is equivalent of &; it’ll return TRUE only if all values of x are TRUE’s.


### 13.4.4 Exercises
1.What will sum(is.na(x)) tell you? How about mean(is.na(x))?

-sum(is.na(x)) will return the number of NAs in x by the number of TRUES. mean() gives the proportion of NAs in x by the form of TRUES

2.What does prod() return when applied to a logical vector? What logical summary function is it equivalent to? What does min() return when applied to a logical vector? What logical summary function is it equivalent to? Read the documentation and perform a few experiments.

-When applied to a lgocial vector, prod() will return the product of all the elements in the vector, treating TRUE as 1 and FALSE as 0. It equals to the & 
-When applied the min() function to a logical vector, it returns FALSE if there are any FALSE values in the vector, and TRUE if all values are TRUE. It's equivalent to using the all() function.

## 13.5 Conditional transformations

### 13.5.1 if_else() 
-In if_else(), the first argument, condition, is a logical vector, the second, true, gives the output when the condition is true, and the third, false, gives the output if the condition is false. There’s an optional fourth argument, missing which will be used if the input is NA.

### 13.5.2 case_when()
-It takes pairs that look like condition ~ output. condition must be a logical vector; when it’s TRUE, output will be used.

### 13.5.3 Compatible types
-Note that both if_else() and case_when() require compatible types in the output. 

### Exerciese
 
1. A number is even if it’s divisible by two, which in R you can find out with x %% 2 == 0. Use this fact and if_else() to determine whether each number between 0 and 20 is even or odd.
```{r}
numbers <- 0:20
result <- if_else(numbers %% 2 == 0, "Even", "Odd")
result
```

2. Given a vector of days like x <- c("Monday", "Saturday", "Wednesday"), use an ifelse() statement to label them as weekends or weekdays.
```{r}
x <- c("Monday", "Saturday", "Wednesday")
result <- ifelse(x %in% c("Saturday", "Sunday"), "Weekend", "Weekday")
result
```

3. Use ifelse() to compute the absolute value of a numeric vector called x.
```{r}
x <- c(-5, 3, -7, 8, -2, 4, 6, -64, 15, -17)
abs_x <- ifelse(x < 0, -x, x)
abs_x
```

4. Write a case_when() statement that uses the month and day columns from flights to label a selection of important US holidays (e.g., New Years Day, 4th of July, Thanksgiving, and Christmas). First create a logical column that is either TRUE or FALSE, and then create a character column that either gives the name of the holiday or is NA.
```{r}
flights %>%
  mutate(
    is_holiday = case_when(
      (month == 1 & day == 1) ~ TRUE,          # New Year's Day
      (month == 7 & day == 4) ~ TRUE,          # 4th of July
      (month == 11 & day == 22) ~ TRUE,       # Thanksgiving
      (month == 12 & day == 25) ~ TRUE,       # Christmas
      TRUE ~ FALSE                            # Non-holiday
    ),
    holiday_name = case_when(
      is_holiday ~ "Holiday",
      TRUE ~ NA_character_))
```

# 14  Numbers

## 14.1 Introduction

-We’ll start by giving you a couple of tools to make numbers if you have strings, and then going into a little more detail of count(). Then we’ll dive into various numeric transformations that pair well with mutate(), including more general transformations that can be applied to other types of vectors, but are often used with numeric vectors. We’ll finish off by covering the summary functions that pair well with summarize() and show you how they can also be used with mutate().

### 14.1.1 Prerequisites
```{r}
library(tidyverse)
library(nycflights13)
```

## 14.2 Making numbers
```{r}
x <- c("1.2", "5.6", "1e3")
parse_double(x)


# Use parse_number() when the string contains non-numeric text that you want to ignore.
x <- c("$1,234", "USD 3,513", "59%")
parse_number(x)

```

## 14.3 Counts
```{r}
#How count() works
flights |> count(dest)

#If you want to see the most common values, add sort = TRUE
flights |> count(dest, sort = TRUE)

#if you want to see all the values, you can use |> View() or |> print(n = Inf).
#You can perform the same computation “by hand” with group_by(), summarize() and n(). 

flights |> 
  group_by(dest) |> 
  summarize(
    n = n(),
    delay = mean(arr_delay, na.rm = TRUE))

#n_distinct(x) counts the number of distinct (unique) values of one or more variables. 
flights |> 
  group_by(dest) |> 
  summarize(carriers = n_distinct(carrier)) |> 
  arrange(desc(carriers))

#A weighted count is a sum. For example you could “count” the number of miles each plane flew:
flights |> 
  group_by(tailnum) |> 
  summarize(miles = sum(distance))

#Weighted counts are a common problem so count() has a wt argument that does the same thing:
flights |> count(tailnum, wt = distance)

#You can count missing values by combining sum() and is.na(). 
flights |> 
  group_by(dest) |> 
  summarize(n_cancelled = sum(is.na(dep_time))) 
```

### 14.3.1 Exercises

1.How can you use count() to count the number rows with a missing value for a given variable?

```{r}
flights |> 
  group_by(dest) |> 
  count(is.na(dep_time))
```

2.Expand the following calls to count() to instead use group_by(), summarize(), and arrange():
flights |> count(dest, sort = TRUE)

flights |> count(tailnum, wt = distance)
```{r}
flights |>
  group_by(dest) |>
  summarize(n = n()) |>
  arrange(desc(n))

flights |>
  group_by(tailnum) |>
  summarize(total_distance = sum(distance, na.rm = TRUE)) |>
  arrange(desc(total_distance))
```

## 14.4 Numeric transformations

### 14.4.1 Arithmetic and recycling rules

1. Explain in words what each line of the code used to generate Figure 14.1 does.
```{r}
flights |>  #Designate flight data set for extracting 
  group_by(hour = sched_dep_time %/% 100) |>  #This line groups the data by the "hour" variable, which is created by dividing the "sched_dep_time" column by 100 (to extract the hour portion of the departure time). 
  summarize(prop_cancelled = mean(is.na(dep_time)), n = n()) |>  # It calculates the proportion of canceled flights (prop_cancelled) using the mean of is.na(dep_time) and also calculates the total count of flights (n) in each group.
  filter(hour > 1) |> # Filter out groups where the "hour" is greater than 1
  ggplot(aes(x = hour, y = prop_cancelled)) + #Plot with "hour" on the x-axis and "prop_cancelled" on the y-axis
  geom_line(color = "grey50") + # This line adds a line layer to the plot, creating a line plot. The "color" parameter sets the color of the line to grey50.
  geom_point(aes(size = n))#  Add point markers to the plot, where the size of the points is mapped to the "n" variable. 
```

2. What trigonometric functions does R provide? Guess some names and look up the documentation. Do they use degrees or radians?

-sin(x): Returns the sine of the angle x, where x is in radians.
-cos(x): Returns the cosine of the angle x, where x is in radians.
-tan(x): Returns the tangent of the angle x, where x is in radians.
-These functions typically use radians for angle measurement. 

3. Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. You can see the basic problem by running the code below: there’s a gap between each hour.
```{r warning=FALSE}
#Original
flights |> 
  filter(month == 1, day == 1) |> 
  ggplot(aes(x = sched_dep_time, y = dep_delay)) +
  geom_point()

#Modification
flights |>
  filter(month == 1, day == 1) |>
  ggplot(aes(x = sched_dep_time, y = dep_delay)) +
  geom_point() +
  scale_x_continuous(breaks = seq(0, 2400, by = 100))
```

4. Convert them to a more truthful representation of time (either fractional hours or minutes since midnight).

Round dep_time and arr_time to the nearest five minutes.
```{r}
# Convert dep_time and arr_time to fractional hours
flights <- flights |>
  mutate(
    dep_time = floor(dep_time / 100) + (dep_time %% 100) / 60,
    arr_time = floor(arr_time / 100) + (arr_time %% 100) / 60
  )

# Round dep_time and arr_time to the nearest five minutes

flights <- flights |>
  mutate(
    dep_time = round(dep_time * 12) / 12,  # 5 minutes per unit
    arr_time = round(arr_time * 12) / 12
  )
```

## 14.5 General transformations

### 14.5.1 Ranks

```{r}
#Note that the smallest values get the lowest ranks; use desc(x) to give the largest values the smallest ranks:
x <- c(1, 2, 2, 3, 4, NA)
min_rank(desc(x))
```
-View documents of  dplyr::row_number(), dplyr::dense_rank(), dplyr::percent_rank(), and dplyr::cume_dist()

### 14.5.2 Offsets

```{r}
#dplyr::lead() and dplyr::lag() allow you to refer the values just before or just after the “current” value. They return a vector of the same length as the input, padded with NAs at the start or end:

x <- c(2, 5, 11, 11, 19, 35)
lag(x)
lead(x)

#x - lag(x) gives you the difference between the current and previous value.
x - lag(x)

#x == lag(x) tells you when the current value changes.
x == lag(x)
```

### 14.5.3 Consecutive identifiers

- cumsum() will increment group by one.
- Another approach for creating grouping variables is consecutive_id(), which starts a new group every time one of its arguments changes. 
- To keep the first row from each repeated x, you could use group_by(), consecutive_id(), and slice_head()

### Exercise

1.Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank().
```{r}
flights |>
  arrange(desc(dep_delay)) |> 
  mutate(rank = min_rank(desc(dep_delay))) |>  
  filter(rank <= 10) 
```

2.Which plane (tailnum) has the worst on-time record?
```{r}
flights |>
  group_by(tailnum) |>
  summarize(average_delay = mean(dep_delay, na.rm = TRUE)) |>
  arrange(average_delay, na.last = TRUE)
```

3.What time of day should you fly if you want to avoid delays as much as possible?
```{r}
flights <- flights |>
  mutate(hour = as.numeric(substring(sched_dep_time, 1, 2)))

average_delay_by_hour <- flights |>
  group_by(hour) |>
  summarize(average_delay = mean(dep_delay, na.rm = TRUE))

average_delay_by_hour |>
  filter(average_delay == min(average_delay))
```

4.What does flights |> group_by(dest) |> filter(row_number() < 4) do? What does flights |> group_by(dest) |> filter(row_number(dep_delay) < 4) do?

-The first line of codes filters the grouped data to keep only the rows where the row number (order within each destination group) is less than 4 by 'dest'
-The second line  filters the data based on the row numbers within each destination group considering the 'dep_delay' column. It selects the first three rows within each destination group

5.For each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.
```{r}
#Calculate the total minutes of delay for each destination
destination_dt <- flights |>
  group_by(dest) |>
  summarize(total_delay = sum(dep_delay, na.rm = TRUE))

#Join the origianl dataset
new_flights <- flights |>
  left_join(destination_dt, by = "dest")

#Calculate the proportion of the total delay for each flight's destination
new_flights |>
  mutate(proportion_of_total_delay = dep_delay / total_delay)
```

6.Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag(), explore how the average flight delay for an hour is related to the average delay for the previous hour.
```{r}
#The original set
flights |> 
  mutate(hour = dep_time %/% 100) |> 
  group_by(year, month, day, hour) |> 
  summarize(
    dep_delay = mean(dep_delay, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) |> 
  filter(n > 5)

#Modified
flights |>
  mutate(hour = dep_time %/% 100) |>
  group_by(year, month, day, hour) |>
  summarize(
    dep_delay = mean(dep_delay, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) |>
  filter(n > 5) |>
  mutate(prev_hour_delay = lag(dep_delay)) |>
  na.omit()
```
-It seemingly the higher previous_hours_delay would cause higher average flight delay for an hour.

7.Look at each destination. Can you find flights that are suspiciously fast (i.e. flights that represent a potential data entry error)? Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?

```{r, include = FALSE, eval = FALSE}
#Find the shortest flight to each destination
shortest_flight <- flights |>
  group_by(dest) |>
  mutate(shortest_time = min(air_time), 
         mean_time = mean(air_time)) |>
  ungroup() |>
  mutate(diff_from_short = air_time-shortest_time, 
         diff_from_mean = air_time-mean_time) |>
  arrange(diff_from_mean) |>
  select(dest, shortest_time, air_time, diff_from_mean, diff_from_short, tailnum)
#Compute the air time of a flight relative to the shortest flight to that destination.
<<<<<<< HEAD
#flights |>
  #left_join(shortest_flight, by = "dest") |> 
  #mutate(relative_air_time = air_time / shortest_time) |>
  #arrange(desc(relative_air_time)) |>
  #head(10)
=======
flights |>
  left_join(shortest_flight, by = "dest") #|> 
  mutate(relative_air_time = air_time / shortest_time) |>
  arrange(desc(relative_air_time)) |>
  head(10)
>>>>>>> 1bd3bd5854f3fb45e97410a13fc1ed6e32fcccfb
```
-Flight N729JB, N531JB, and N566JB are the top three most delayed in the air ?

8.Find all destinations that are flown by at least two carriers. Use those destinations to come up with a relative ranking of the carriers based on their performance for the same destination.
```{r}
#Find all destinations that are flown by at least two carriers.
destinations_with_twomore_carriers <- flights |>
  group_by(dest) |>
  mutate(carrier_count = n_distinct(carrier)) |>
  filter(carrier_count >= 2) |>
  distinct(dest)

#Lets see the relative ranking of the carriers based on their performance for the same destination.
flights |>
  filter(dest %in% destinations_with_twomore_carriers$dest) |>
  group_by(carrier, dest) |>
  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) |>
  group_by(carrier) |>
  summarize(relative_rank = mean(avg_dep_delay, na.rm = TRUE)) |>
  arrange(relative_rank)
```
-Seemingly the UA has the worst performance for the same destination.

## 14.6 Numeric summaries

### 14.6.1 Center

-mean(), median()

### 14.6.2 Minimum, maximum, and quantiles

-min() and max() will give you the largest and smallest values.

-quantile() is a generalization of the median: quantile(x, 0.25) will find the value of x that is greater than 25% of the values, quantile(x, 0.5) is equivalent to the median, and quantile(x, 0.95) will find the value that’s greater than 95% of the values.

### 14.6.3 Spread

-IQR() might be new — it’s quantile(x, 0.75) - quantile(x, 0.25) and gives you the range that contains the middle 50% of the data.

### 14.6.4 Distributions

-geom_freqpoly() can help create distribution

### 14.6.5 Positions

-Extracting a value at a specific position: first(x), last(x), and nth(x, n)
-Because dplyr functions use _ to separate components of function and arguments names, these functions use na_rm instead of na.rm.

### 14.6.6 With mutate()

-x / sum(x) calculates the proportion of a total.
-(x - mean(x)) / sd(x) computes a Z-score (standardized to mean 0 and sd 1).
-(x - min(x)) / (max(x) - min(x)) standardizes to range [0, 1].
-x / first(x) computes an index based on the first observation.


### 14.6.7 Exercises (WARN)

1. Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. When is mean() useful? When is median() useful? When might you want to use something else? Should you use arrival delay or departure delay? Why might you want to use data from planes?

-When is mean() useful?: when I want to get an overall sense of the typical delay in a group of flights, and understand the central tendency of the data

- When is median() useful?: In the case we want to assess the central value that separates the higher half of delays from the lower half.

-When might you want to use something else? :When I want to specify the data, like checking calculating quantiles (e.g., 25th percentile, 75th percentile) or percentiles to pick up performance on specific flight.

2. Which destinations show the greatest variation in air speed?
```{r}
flights |>
  group_by(dest) |>
  summarize(variation = sd(distance/air_time, na.rm = TRUE)) |>
  arrange(desc(variation)) |>
  head(5)
```
-The OKC shows the greatest variation in air speed

3.Create a plot to further explore the adventures of EGE. Can you find any evidence that the airport moved locations? Can you find another variable that might explain the difference? (Why this is empty?)

```{r，message = FALSE}
EGE_flights <- flights |>
  filter(dest == "EGE")

EGE_flights |>
  group_by(year) |>
  summarize(num_flights = n()) |>
  ggplot(aes(x = year, y = num_flights)) +
  geom_line() +
  labs(x = "Year",
       y = "Number of Flights")
```


# 15  Strings

## 15.1.1 Prerequisites
```{r}
library(tidyverse)
library(babynames)
```

## 15.2 Creating a string

-You can create a string using either single quotes (') or double quotes ("). There’s no difference in behavior between the two, so in the interests of consistency, the tidyverse style guide recommends using ", unless the string contains multiple ".

### 15.2.1 Escapes

```{r}
#To include a literal single or double quote in a string, you can use \ to “escape” it:
double_quote <- "\"" # or '"'
single_quote <- '\'' # or "'"
double_quote
single_quote

#So if you want to include a literal backslash in your string, you’ll need to escape it: "\\":
backslash <- "\\"
backslash

#To see the raw contents of the string, use str_view()
x <- c(single_quote, double_quote, backslash)
x

str_view(x)
```

### 15.2.2 Raw strings

-A raw string usually starts with r"( and finishes with )". But if your string contains )" you can instead use r"[]" or r"{}", and if that’s still not enough, you can insert any number of dashes to make the opening and closing pairs unique, e.g., r"--()--", r"---()---", etc.

### 15.2.3 Other special characters

-The most common are \n, a new line, and \t, tab. You’ll also sometimes see strings containing Unicode escapes that start with \u or \U.

### 15.2.4 Exercises

1. Create strings that contain the following values:

He said "That's amazing!"

\a\b\c\d

\\\\\\

```{r}
t1524 <- r"('He said "That's amazing!"'
"\a\b\c\d"
"\\\\\\")" 
t1524
str_view(t1524)
```

2. Create the string in your R session and print it. What happens to the special “\u00a0”? How does str_view() display it? Can you do a little googling to figure out what this special character is?

```{r}
x <- "This\u00a0is\u00a0tricky"
x
str_view(x)

#lets try
x <- c("This", "\u00a0", "is", "\u00a0", "tricky")
x
```
-“\u00a0” does not generate results, and it is NO-BREAK SPACE!

## 15.3 Creating many strings from data

### 15.3.1 str_c()

-str_c() takes any number of vectors as arguments and returns a character vector

```{r}
#If you want missing values to display in another way, use coalesce() to replace them. Depending on what you want, you might use it either inside or outside of str_c():
df <- tibble(name = c("Flora", "David", "Terra", NA))
df |> 
  mutate(
    greeting1 = str_c("Hi ", coalesce(name, "you"), "!"),
    greeting2 = coalesce(str_c("Hi ", name, "!"), "Hi!")
  )
```


### 15.3.2 str_glue()

- str_glue() converts missing values to the string "NA".

### 15.3.3 str_flatten()

-  str_flatten() takes a character vector and combines each element of the vector into a single string, and work well with summarize()

### 15.3.4 Exercises

1.Compare and contrast the results of paste0() with str_c() for the following inputs:

```{r}
str_c("hi ", NA)
paste0("hi ", NA)

paste0(letters[1:2], letters[1:3])

```
In the first case, paste9() treat NA as a string, and return "hi NA". In the second case, In the second case, the str_c() cannot recycle the designated values in letters.

2. What’s the difference between paste() and paste0()? How can you recreate the equivalent of paste() with str_c()?

-?paste(), ?paste0()
-paste0() is similar to paste(), but it has no separator. The return of paste0() will have no blank between values.

3. Convert the following expressions from str_c() to str_glue() or vice versa:

str_c("The price of ", food, " is ", price)

str_glue("I'm {age} years old and live in {country}")

str_c("\\section{", title, "}")


-food <- c('food')
-price <-c('price')
-age <- c('age')
-country <- c('country')

-str_glue("The price of {food} is {price}")

-str_c("I'm ", age, " years old and live in ", country)

-str_glue("\\section{{{title}}}")


## 15.4 Extracting data from strings

### 15.4.1 Separating into rows

-The most common case is requiring separate_longer_delim() to split based on a delimiter.

-separate_longer_position() is suitable for dataset with a very compact format where each character is used to record a value

## 15.4.2 Separating into columns

-separate_wider_delim() can separate a string into columns, but it needs the delimiter and the names in the arguments.

-In the argument, you can use an NA name to omit it from results.

-separate_wider_position() works a little differently because you typically want to specify the width of each column. So you give it a named integer vector, where the name gives the name of the new column, and the value is the number of characters it occupies. You can omit values from the output by not naming them

## 15.4.3 Diagnosing widening problems

-separate_wider_delim() provides two arguments to help if some of the rows don’t have the expected number of pieces: too_few and too_many.

- too_few = "debug" to ensure that new problems become errors. too_few = "align_start" and too_few = "align_end" fill in the missing pieces with NAs and move on.


## 15.5 Letters

### 15.5.1 Length

-str_length() tells you the number of letters in the string

### 15.5.2 Subsetting

-You can extract parts of a string using str_sub(string, start, end), where start and end are the positions where the substring should start and end. 

### Exercises (WARN)

1. We could use str_sub() with mutate() to find the first and last letter of each name (dont forget place the position of rows.)

```{r}
babynames <- babynames::babynames
babynames |>
  mutate(
    first_letter = str_sub(name, 1, 1),
    last_letter = str_sub(name, -1, -1)
  )
```
2. When computing the distribution of the length of babynames, why did we use wt = n?
Use str_length() and str_sub() to extract the middle letter from each baby name. What will you do if the string has an even number of characters?

-We use wt = n becasue it is a simple way to  count the occurrences of each unique name of babies (?)
```{r}
babynames |>
  mutate(
    middle_letter = ifelse(str_length(name) %% 2 == 1, 
                            str_sub(name, str_length(name) %/% 2 + 1,
                                    str_length(name) %/% 2 + 1),
                           str_sub(name, str_length(name) %/% 2,
                                   str_length(name) %/% 2 + 1) ))
```

3. Are there any major trends in the length of babynames over time? What about the popularity of first and last letters?

```{r}

#Part 1
library(babynames)
babynames1 <- babynames |>
  group_by(year) |>
  mutate(average_name_length = mean(nchar(name)))

ggplot(data = babynames1, aes(x = year, y = average_name_length)) +
  geom_line() +
  labs(x = "Year", y = "Average Name Length") +
  ggtitle("Trends in the Length of Baby Names Over Time")


#Part 2
babynames2 <- babynames |>
  mutate(first_letter = str_sub(name, 1, 1),
         last_letter = str_sub(name, -1, -1)) |>
  group_by(first_letter) |>
  mutate(first_letter_count = n()) |>
  group_by(last_letter) |>
  mutate(last_letter_count = n())

ggplot(data = babynames2, aes(x = first_letter, y = first_letter_count)) +
  geom_bar(stat = "identity") +
  labs(x = "First Letter", y = "Count") +
  ggtitle("Popularity of First Letters")

ggplot(data = babynames2, aes(x = last_letter, y = last_letter_count)) +
  geom_bar(stat = "identity") +
  labs(x = "Last Letter", y = "Count") +
  ggtitle("Popularity of Last Letters")
```

## 15.6 Non-English text

### 15.6.1 Encoding

-encoding = () add certain names of languages.

### 15.6.2 Letter variations

-Working in languages with accents poses a significant challenge when determining the position of letters (e.g., with str_length() and str_sub())

-Note that a comparison of these strings with == interprets these strings as different, while the handy str_equal() function in stringr recognizes that both have the same appearance

-locale = can help adapt different languages' unique formats.

# 16  Regular expressions

## 16.1 Introduction

```{r}
library(tidyverse)
library(babynames)
```

## 16.3 Key functions

### 16.3.1 Detect matches

-str_detect() returns a logical vector that is TRUE if the pattern matches an element of the character vector and FALSE otherwise

### 16.3.2 Count matches

-str_count()tells you how many matches there are in each string.

- str_to_lower() convert words to lower case.

### 16.3.3 Replace values

-str_replace() replaces the first match, and as the name suggests, str_replace_all() replaces all matches

-str_remove() and str_remove_all() are handy shortcuts for str_replace(x, pattern, "")

### 16.3.4 Extract variables

-To extract this data using separate_wider_regex() we just need to construct a sequence of regular expressions that match each piece. If we want the contents of that piece to appear in the output, we give it a name. 
"separate_wider_regex(
    str,
    patterns = c(
      "<", 
      name = "[A-Za-z]+", 
      ">-", 
      gender = ".",
      "_",
      age = "[0-9]+"
    )
  )"
  
### 16.3 Exercises (warn)

1.What baby name has the most vowels? What name has the highest proportion of vowels? (Hint: what is the denominator?)
```{r}
#Baby name with the most vowels
babynames |>
  mutate(vowel_count = str_count(name, "[aeiouAEIOU]")) |>
  filter(vowel_count == max(vowel_count)) |>
  distinct(name)
#Baby name has the highest proportion of vowels
babynames |>
  mutate(vowel_count = str_count(name, "[aeiouAEIOU]"))|>
  mutate(vowel_proportion = vowel_count / nchar(name)) |>
  filter(vowel_proportion == max(vowel_proportion)) |>
  select(name, vowel_proportion)
```

2.Replace all forward slashes in "a/b/c/d/e" with backslashes. What happens if you attempt to undo the transformation by replacing all backslashes with forward slashes? (We’ll discuss the problem very soon.)
```{r}
original<- "F/-/1/5/E"
replaced<- gsub("/", "\\", original)
undo_string <- gsub("\\\\", "/", replaced)
replaced
undo_string
```
-Nothing changes when attempted to undo the transformation by replacing all backslashes with forward slashes.

3.Implement a simple version of str_to_lower() using str_replace_all().

```{r}
replacements <- c(
  "A" = "a", "B" = "b", "C" = "c", "D" = "d", "E" = "e",
  "F" = "f", "G" = "g", "H" = "h", "I" = "i", "J" = "j",
  "K" = "k", "L" = "l", "M" = "m", "N" = "n", "O" = "o",
  "P" = "p", "Q" = "q", "R" = "r", "S" = "s", "T" = "t",
  "U" = "u", "V" = "v", "W" = "w", "X" = "x", "Y" = "y",
  "Z" = "z"
)
lower_words <- str_replace_all(words, pattern = replacements)
head(lower_words)
```

4.Create a regular expression that will match telephone numbers as commonly written in your country.

```{r}
x <- c("13562475567")
str_view(x, "\\d{3}-\\d{4}-\\d{4}")
```

## 16.4 Pattern details

### 16.4.1 Escaping

-We use strings to represent regular expressions, and \ is also used as an escape symbol in strings. So to create the regular expression \. we need the string "\\."

### 16.4.2 Anchors

-Anchor the regular expression using ^ to match the start or $ to match the end

-To force a regular expression to match only the full string, anchor it with both ^ and $

-Match the boundary between words (i.e. the start or end of a word) with \b

### 16.4.3 Character classes

-There are many pairs for characters(cannot remember them all by now).

### 16.4.4 Quantifiers

-{n} matches exactly n times.

-{n,} matches at least n times.

-{n,m} matches between n and m times.

### 16.4.5 Operator precedence and parentheses

-quantifiers have high precedence and alternation has low precedence which means that ab+ is equivalent to a(b+), and ^a|b$ is equivalent to (^a)|(b$). 
### 16.4.6 Grouping and capturing
-\1 refers to the match contained in the first parenthesis, \2 in the second parenthesis, and so on.


### 16.4.7 Exercises

1.How would you match the literal string "'\? How about "$^$"?

For "'\:  \"\\'\\\\,  \\' to express', \\\\ to express \
For "$^$": \\$\\^\\$

2.Explain why each of these patterns don’t match a \: "\", "\\", "\\\".

This is because the backslash character is a special character in regular expressions and needs to be escaped to be treated as a literal character.

For "\": treat backslash as an escape character rather than a literal backslash

For "\\": the backslash character is not escaped, so the regular expression engine interprets it as an escape character and expects another character to follow it. But there is no following. 

For "\\\": The first backslash "\" is used to escape the second backslash "\". This means the pattern is looking for a literal backslash. But "\\" xpects another character to follow it, while there is no following.

3.Given the corpus of common words in stringr::words, create regular expressions that find all words that:

a.Start with “y”.
("^y")

b.Don’t start with “y”.
("^[^y]")

c.End with “x”.
("x$")

d.Are exactly three letters long. (Don’t cheat by using str_length()!) 
("^[a-z]{3}$")

e.Have seven letters or more.
("^[a-z]{7,}$")

f.Contain a vowel-consonant pair.
("[aeiou][^aeiou]")

g.Contain at least two vowel-consonant pairs in a row.
("[aeiou][^aeiou][aeiou][^aeiou]")

h.Only consist of repeated vowel-consonant pairs.
("^(?:[aeiou][^aeiou])+$")

4.Create 11 regular expressions that match the British or American spellings for each of the following words: airplane/aeroplane, aluminum/aluminium, analog/analogue, ass/arse, center/centre, defense/defence, donut/doughnut, gray/grey, modeling/modelling, skeptic/sceptic, summarize/summarise. Try and make the shortest possible regex!

a(ero)?plane

alumin(ium|um)

analog(ue)?

ar?se

cent(re|er)

defen(s|c)e

d(ough)?nut

gr(a|e)y

model(ling)?

ske(ptic|ptical)?

summar(ize|ise)


5.Switch the first and last letters in words. Which of those strings are still words?
```{r}
switched <- str_replace(words, "^(.)(.*)(.)$", "\\3\\2\\1")
words[words %in% switched]
```

6.Describe in words what these regular expressions match: (read carefully to see if each entry is a regular expression or a string that defines a regular expression.)

a. ^.*$
This expression matches any string or line that contains any character, even it is a line or empty.

b. "\\{.+\\}"
This regular expression matches a string that contains a pair of curly braces (i.e., { and }) with one or more characters in between.
 
c. \d{4}-\d{2}-\d{2}
This regular expression matches a date format in the form of "YYYY-MM-DD," where \d represents a digit (0-9).

d. "\\\\{4}"
This regular expression matches the literal string "{4}" within double quotes. It looks for the exact sequence of characters, including the escape character \.

e. \..\..\..
This regular expression matches a string containing three periods (dots), separated by any character.

f. (.)\1\1
This regular expression matches any character followed by two identical characters. The (.) captures any character, and \1\1 checks if the next two characters are the same as the first character captured.

g. "(..)\\1"
his regular expression matches a string enclosed in double quotes that consists of two identical characters followed by another two identical characters. It captures the first two characters and checks if the next two are the same.

7.Solve the beginner regexp crosswords at https://regexcrossword.com/challenges/beginner.


## 16.5 Pattern control

### 16.5.1 Regex flags

- The most useful flag is probably ignore_case = TRUE because it allows characters to match either their uppercase or lowercase forms

- dotall = TRUE lets . match everything, including \n

- multiline = TRUE makes ^ and $ match the start and end of each line rather than the start and end of the complete string

-comments = TRUE tweaks the pattern language to ignore spaces and new lines, as well as everything after #. This allows you to use comments and whitespace to make complex regular expressions more understandable

### 16.5.2 Fixed matches

-You can opt-out of the regular expression rules by using fixed()

-fixed() also gives you the ability to ignore case

-If you’re working with non-English text, you will probably want coll() instead of fixed(), as it implements the full rules for capitalization as used by the locale you specify. 

## 16.6 Practice

### 16.6.1 Check your work

- Step by step construct your pattern in finding the target matches.

- And pay attention to the details in overlapping other strings.

### 16.6.2 Boolean operations

- Imagine we want to find words that only contain consonants. One technique is to create a character class that contains all letters except for the vowels ([^aeiou]), then allow that to match any number of letters ([^aeiou]+), then force it to match the whole string by anchoring to the beginning and the end (^[^aeiou]+$)

- But you can make this problem a bit easier by flipping the problem around. Instead of looking for words that contain only consonants, we could look for words that don’t contain any vowels

-If you get stuck trying to create a single regexp that solves your problem, take a step back and think if you could break the problem down into smaller pieces, solving each challenge before moving onto the next one.


### 16.6.3 Creating a pattern with code

-create the pattern from the vector using str_c() and str_flatten()

-whenever you create patterns from existing strings it’s wise to run them through str_escape() to ensure they match literally

### 16.6.4 Exercises (Q)

1.For each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str_detect() calls.

a.Find all words that start or end with x.
-str_extract_all(something, "\\b\\w*x\\w*\\b")[[1]]

b.Find all words that start with a vowel and end with a consonant.
-str_detect(something, "^x|[^x]$")
pattern <- "\\b[aeiouAEIOU][a-zA-Z]*[^aeiouAEIOU\\W]\\b"

c.Are there any words that contain at least one of each different vowel?
pattern <- "\\b(?=.*a)(?=.*e)(?=.*i)(?=.*o)(?=.*u)\\w+\\b"

2.Construct patterns to find evidence for and against the rule “i before e except after c”?
-pattern_for_ie_after_c <- "\\b\\w*cie\\w*\\b"
-pattern_for_cei <- "\\b\\w*[^c]cei\\w*\\b"


3.colors() contains a number of modifiers like “lightgray” and “darkblue”. How could you automatically identify these modifiers? (Think about how you might detect and then removed the colors that are modified).

4.Create a regular expression that finds any base R dataset. You can get a list of these datasets via a special use of the data() function: data(package = "datasets")$results[, "Item"]. Note that a number of old datasets are individual vectors; these contain the name of the grouping “data frame” in parentheses, so you’ll need to strip those off.

-base_datasets <- data(package = "datasets")$results[, "Item"]

-regex_pattern <- "^(\\w+)(\\s*\\(.*\\))?$"

-matched_datasets <- character()

## 16.7 Regular expressions in other places

### 16.7.1 tidyverse

-There are three other particularly useful places where you might want to use a regular expressions

-matches(pattern) will select all variables whose name matches the supplied pattern. 

-pivot_longer()'s names_pattern argument takes a vector of regular expressions, just like separate_wider_regex(). It’s useful when extracting data out of variable names with a complex structure

-The delim argument in separate_longer_delim() and separate_wider_delim() usually matches a fixed string, but you can use regex() to make it match a pattern. 

### 16.7.2 Base R

-apropos(pattern) searches all objects available from the global environment that match the given pattern. 

# 17  Factors

## 17.2 Factor basics

-Create a list of the valid levels, and then create a factor following these valid levels.

-If you omit the levels, they’ll be taken from the data in alphabetical order

-Sorting alphabetically is slightly risky because not every computer will sort strings in the same way. So forcats::fct() orders by first appearance

-If you ever need to access the set of valid levels directly, you can do so with levels()

## 17.3 General Social Survey

1.Explore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot?

```{r}
ggplot(gss_cat, aes(rincome)) +
  geom_bar() +
  scale_x_discrete(drop = FALSE)
```
The default bar chart's x-axis is unreadable for overlapping labels.

```{r}
#Switch around and do scale_x_discrete
ggplot(gss_cat, aes(rincome)) +
  geom_bar() +
  scale_x_discrete(drop = FALSE) +
  coord_flip()
```
2.What is the most common relig in this survey? What’s the most common partyid?

```{r}
#Most common relig
gss_cat %>%
  count(relig) %>%
  arrange(-n) %>%
  head(3)

#Most common partyid
gss_cat %>%
  count(partyid) %>%
  arrange(-n) %>%
  head(3)
```

3.Which relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualization?

```{r}
#Which relig does denom (denomination) apply to
levels(gss_cat$denom)

#How can you find out with a table
gss_cat %>%
  filter(!denom %in% c("No answer", "Other", "Don't know", "Not applicable",   "No denomination")) %>%
  count(relig)

#How can you find out with a visualization
gss_cat %>%
  count(relig, denom) %>%
  ggplot(aes(x = relig, y = denom, size = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90))

```

## 17.4 Modifying factor order

1. There are some suspiciously high numbers in tvhours. Is the mean a good summary?
```{r}
gss_cat %>%
  filter(!is.na(tvhours)) %>%
  ggplot(aes(x = tvhours)) +
  geom_histogram(binwidth = 1)
```
There are some outliners in tvhours. It is better to use median instead of mean.

2. For each factor in gss_cat identify whether the order of the levels is arbitrary or principled.
```{r}

levels(gss_cat$marital)
#marital is arbitrary

levels(gss_cat$race)
#race is arbitrary

levels(gss_cat$rincome)
#rincome is principled

levels(gss_cat$partyid)
#partyid is arbitrary

levels(gss_cat$relig)
#relig is arbitrary

levels(gss_cat$denom)
#denom is arbitrary
```

3. Why did moving “Not applicable” to the front of the levels move it to the bottom of the plot?

-Because it is determined by the factor level. Combine fct_relevel() and “Not applicable” to the front will subsequently move “Not applicable” to the bottom of the plot.


## 17.5 Modifying factor levels

-fct_recode() will leave the levels that aren’t explicitly mentioned as is, and will warn you if you accidentally refer to a level that doesn’t exist.

-If you want to collapse a lot of levels, fct_collapse() is a useful variant of fct_recode()

-Sometimes you just want to lump together the small groups to make a plot or table simpler. That’s the job of the fct_lump_*() family of functions. fct_lump_lowfreq() is a simple starting point that progressively lumps the smallest groups categories into “Other”, always keeping “Other” as the smallest category.


1. How have the proportions of people identifying as Democrat, Republican, and Independent changed over time?

```{r}
gss_cat %>%
  mutate(partyid = fct_collapse(partyid,
    other = c("No answer", "Don't know", "Other party"),
    rep = c("Strong republican", "Not str republican"),
    ind = c("Ind,near rep", "Independent", "Ind,near dem"),
    dem = c("Not str democrat", "Strong democrat"))) |>
  group_by(year, partyid) |>
  summarize(n = n()) |>
  ggplot(mapping = aes(x = year, y = n, color = fct_reorder2(partyid, year, n))) +
  geom_point() +
  geom_line() +
  labs(color = 'Party',
       x = 'Year',
       y = 'Count')
```
The general trends of changing identity are similar within three groups, but Independent has the largest volume of changes.

2. How could you collapse rincome into a small set of categories?
```{r}
gss_cat |>
  mutate(rincome = fct_collapse(rincome,
    "No answer" = c("No answer", "Don't know", "Refused"),
    "$0 to 5000" = c("Lt $1000", "$1000 to 3000", "$3001 to 4000", "$4001 to 5000"),
    "$5001 to 10000" = c("$5001 to 6000", "$6001 to 7000",
                        "$7001 to 8000", "$8001 to 10000"))) |>
  count(rincome)
```

3. Notice there are 9 groups (excluding other) in the fct_lump example above. Why not 10? (Hint: type ?fct_lump, and find the default for the argument other_level is “Other”.)

The fct_lump function was applied to a factor variable with 10 original levels. Since the default other_level value is "Other," it combines the less frequent levels into a single "Other" level. As a result, WE have 9 groups (including the "Other" group) instead of the original 10 distinct levels.

## 17.6 Ordered factors

-Ordered factors, created with ordered(), imply a strict ordering and equal distance between levels: the first level is “less than” the second level by the same amount that the second level is “less than” the third level, and so on.

# 18  Dates and times

```{r}
library(tidyverse)
library(nycflights13)
```

## 18.2 Creating date/times

### 18.2.1 During import
-If your CSV contains an ISO8601 date or date-time, you don’t need to do anything; readr will automatically recognize it

-For other date-time formats, you’ll need to use col_types plus col_date() or col_datetime() along with a date-time format. 

-If you’re using %b or %B and working with non-English dates, you’ll also need to provide a locale(). See the list of built-in languages in date_names_langs(), or create your own with date_names(),

### 18.2.2 From strings

- Identify the order in which year, month, and day appear in your dates, then arrange “y”, “m”, and “d” in the same order. That gives you the name of the lubridate function that will parse your date.

- ymd() and friends create dates.

- To create a date/time from this sort of input, use make_date() for dates, or make_datetime() for date-times

### 18.2.4 From other types

-You may want to switch between a date-time and a date. That’s the job of as_datetime() and as_date():

-Sometimes you’ll get date/times as numeric offsets from the “Unix Epoch”, 1970-01-01. If the offset is in seconds, use as_datetime(); if it’s in days, use as_date().

### 18.2.5 Exercises

1. What happens if you parse a string that contains invalid dates?

```{r}
ymd(c("2010-10-10", "bananas"))
```
-It will report a failed to parse.

2. What does the tzone argument to today() do? Why is it important?

-It is a character vector specifying which time zone you would like the current time in. It is important since different time-zones can have different dates, and tzone can help us specify the time.

3. For each of the following date-times, show how you’d parse it using a readr column specification and a lubridate function.

d1 <- "January 1, 2010"
d2 <- "2015-Mar-07"
d3 <- "06-Jun-2017"
d4 <- c("August 19 (2015)", "July 1 (2015)")
d5 <- "12/30/14" # Dec 30, 2014
t1 <- "1705"
t2 <- "11:15:10.12 PM"
```{r, warning=FALSE}
library(lubridate)

d1 <- "January 1, 2010"
parse_date(d1, format = "%B %d, %Y")

d2 <- "2015-Mar-07"
parse_date(d2, format = "%Y-%b-%d")

d3 <- "06-Jun-2017"
parse_date(d3, format = "%d-%b-%Y")

d4 <- c("August 19 (2015)", "July 1 (2015)")
parse_date(d4, format = "%B %d (%Y)")

d5 <- "12/30/14"
parsed_date5 <- parse_date(d5, format = "%m/%d/%y")

t1 <- "1705"
parsed_time1 <- hms(t1)

t2 <- "11:15:10.12 PM"
parsed_time2 <- hms(paste(t2, "12"))
```
## 18.3 Date-time components

### 18.3.1 Getting components

-You can pull out individual parts of the date with the accessor functions year(), month(), mday() (day of the month), yday() (day of the year), wday() (day of the week), hour(), minute(), and second(). These are effectively the opposites of make_datetime().

-For month() and wday() you can set label = TRUE to return the abbreviated name of the month or day of the week. Set abbr = FALSE to return the full name.

-We can use wday() to see that more flights depart during the week than on the weekend

### 18.3.2 Rounding

-An alternative approach to plotting individual components is to round the date to a nearby unit of time, with floor_date(), round_date(), and ceiling_date(). Each function takes a vector of dates to adjust and then the name of the unit to round down (floor), round up (ceiling), or round to.

### 18.3.3 Modifying components

-Alternatively, rather than modifying an existing variable, you can create a new date-time with update()

### 18.3.4 Exercises (Q)

1.How does the distribution of flight times within a day change over the course of the year?

```{r}
#Preparation
flights <- nycflights13::flights
flights |> 
  select(year, month, day, hour, minute) |> 
  mutate(departure = make_datetime(year, month, day, hour, minute))

make_datetime_100 <- function(year, month, day, time) {
  make_datetime(year, month, day, time %/% 100, time %% 100)
}


str(flights$year)
str(flights$month)
str(flights$day)
str(flights$dep_time)

flights_dt <- flights |> 
  filter(!is.na(dep_time), !is.na(arr_time)) |> 
  mutate(
    dep_time = as.integer(dep_time),
    arr_time = as.integer(arr_time),
    dep_time = make_datetime_100(year, month, day, dep_time),
    arr_time = make_datetime_100(year, month, day, arr_time),
    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
  ) |> 
  select(origin, dest, ends_with("delay"), ends_with("time"))

flights_dt
```

```{r}
#Plot
flights_dt |>
  filter(!is.na(dep_time)) |>
  mutate(dep_hour = update(dep_time, yday = 1)) |>
  mutate(month = factor(month(dep_time))) |>
  ggplot(aes(x=dep_hour, group = month, color = month))+
  geom_freqpoly(binwidth = 200 * 200)
  mutate(dep_day_of_year = yday(dep_time)) |>
  group_by(dep_day_of_year) |>
  summarize(y = mean(air_time, na.rm = TRUE)) |>
  ggplot(aes(x = dep_day_of_year, y = y))+
  geom_bar(stat = "identity")

```

2.Compare dep_time, sched_dep_time and dep_delay. Are they consistent? Explain your findings.
```{r}
flights_dt |> 
  select(contains('dep')) |>
  mutate(cal_delay = as.numeric(dep_time - sched_dep_time) / 60) |>
  filter(dep_delay != cal_delay)
```
-They are not consistent. There are existing minor time difference between scheduled departure time and departure time. Such difference can be explained by the delay time, which is divert from scheduled time. 

3.Compare air_time with the duration between the departure and arrival. Explain your findings. (Hint: consider the location of the airport.) (Why duration is ZERO?)
```{r}
flights_dt |>
  mutate(
    flight_duration = as.numeric(arr_time - dep_time),
    air_time_mins = air_time,
    diff = flight_duration - air_time_mins
  ) |>
  select(origin, dest, flight_duration, air_time_mins, diff)
```

4.How does the average delay time change over the course of a day? Should you use dep_time or sched_dep_time? Why?
```{r, message=FALSE}
#dep_time
flights_dt |>
  mutate(sched_dep_hour = hour(dep_time)) |>
  group_by(dep_time) |>
  summarise(dep_delay = mean(dep_delay)) |>
  ggplot(aes(y = dep_delay, x = dep_time)) +
  geom_point() +
  geom_smooth()

#sched_dep_hour
flights_dt |>
  mutate(sched_dep_hour = hour(sched_dep_time)) |>
  group_by(sched_dep_hour) |>
  summarise(dep_delay = mean(dep_delay)) |>
  ggplot(aes(y = dep_delay, x = sched_dep_hour)) +
  geom_point() +
  geom_smooth()
```
-We use sched_dep_time since the dep_time will generate biased delays to later in the day.

5.On what day of the week should you leave if you want to minimise the chance of a delay?
```{r}
flights_dt |>
  mutate(weekday = wday(sched_dep_time, label = TRUE)) |>
  group_by(weekday) |>
  summarize(avg_dep_delay = mean(dep_delay, na.rm = TRUE),
            avg_arr_delay = mean(arr_delay, na.rm = TRUE)) |>
  gather(key = 'delay', value = 'minutes', 2:3) |>
  ggplot() +
  geom_col(mapping = aes(x = weekday, y = minutes, fill = delay),
           position = 'dodge')
```
-Looks like Saturday is the best day for a flight.

6.What makes the distribution of diamonds$carat and flights$sched_dep_time similar?
```{r}
#The distribution of diamonds
diamonds |>
  ggplot() +
  geom_freqpoly(mapping = aes(x = carat), binwidth = .04)

#The distribution of flights
flights_dt |>
  mutate(minutes = minute(sched_dep_time)) |>
  ggplot() +
  geom_freqpoly(mapping = aes(x = minutes), binwidth = 1)
```
-It might be that the human factor caused this similarity for the "nice" dataset(?)

7.Confirm our hypothesis that the early departures of flights in minutes 20-30 and 50-60 are caused by scheduled flights that leave early. Hint: create a binary variable that tells you whether or not a flight was delayed.

```{r}
flights_dt |>
  mutate(delayed = dep_delay > 0,
         minutes = minute(sched_dep_time) %/% 10 * 10,
         minutes = factor(minutes, levels = c(0,10,20,30,40,50),
                          labels = c('0 - 9 mins',
                                     '10 - 19 mins',
                                     '20 - 29 mins',
                                     '30 - 39 mins',
                                     '40 - 49 mins',
                                     '50 - 50 mins'))) |>
  group_by(minutes) |>
  summarize(prop_early = 1 - mean(delayed, na.rm = TRUE)) |>
  ggplot() +
  geom_point(mapping = aes(x = minutes, y = prop_early)) +
  labs(x = 'Scheduled departure (minutes)',
       y = 'Proportion of early departures')
```

## 18.4 Time spans

### 18.4.1 Durations

-A difftime class object records a time span of seconds, minutes, hours, days, or weeks.

### 18.4.2 Periods

-Periods are time spans but don’t have a fixed length in seconds, instead they work with “human” times, like days and months.

### 18.4.3 Intervals

We can create an interval by writing start %--% end

### 18.4.4 Exercises (Q, 4)

1.Explain days(!overnight) and days(overnight) to someone who has just started learning R. What is the key fact you need to know?

-Well, overnight itself is a boolean variable. So, days(!overnight) means overnight is FALSE, and the flight arrive on the same day. days(overnight) means overnight is TRUE, and will add one day to the arr_time and sched_arr_time datetime.

2.Create a vector of dates giving the first day of every month in 2015. Create a vector of dates giving the first day of every month in the current year.

```{r}
year_2015 <- years(2015) + months(c(1:12)) + days(1)
year_2015

year_current <- years(year(today())) + months(c(1:12)) + days(1)
year_current
```

3.Write a function that given your birthday (as a date), returns how old you are in years.

```{r}
howold <- function(d) {
  age <- today() - d
  return(floor(age/dyears(1)))
}

howold(ymd(19980419))
```
4.Why can’t (today() %--% (today() + years(1))) / months(1) work? (?)

```{r}
(today() %--% (today() + years(1))) / months(1)

(today() %--% (today() + years(1))) / months(1)
```

## 18.5 Time zones

-Use Sys.timezone() to find current time zone.

-OlsonNames() provides all time zones.

-Change time zones:1. Keep the instant in time the same, and change how it’s displayed. Use this when the instant is correct, but you want a more natural display. 2. Change the underlying instant in time. Use this when you have an instant that has been labelled with the incorrect time zone, and you need to fix it.

# 19  Missing values

## 19.2 Explicit missing values

### 19.2.1 Last observation carried forward

-When data is entered by hand, missing values sometimes indicate that the value in the previous row has been repeated (or carried forward)

-We can fill in these missing values with tidyr::fill(). It works like select(), taking a set of columns.

### 19.2.2 Fixed values

-Some times missing values represent some fixed and known value, most commonly 0. You can use dplyr::coalesce() to replace them.

-If possible, handle this when reading in the data, for example, by using the na argument to readr::read_csv(), e.g., read_csv(path, na = "99"). If you discover the problem later, or your data source doesn’t provide a way to handle it on read, you can use dplyr::na_if()

### 19.2.3 NaN

-a NaN (pronounced “nan”), or not a number; generally behaves just like NA. In the rare case you need to distinguish an NA from a NaN, you can use is.nan(x).

## 18.3 Implicit missing values

-An explicit missing value is the presence of an absence.

-An implicit missing value is the absence of a presence.

### 18.3.1 Pivoting

-Making data wider can make implicit missing values explicit because every combination of the rows and new columns must have some value. 

-By default, making data longer preserves explicit missing values, but if they are structurally missing values that only exist because the data is not tidy, you can drop them (make them implicit) by setting values_drop_na = TRUE.

### 18.3.2 Complete

-tidyr::complete() allows you to generate explicit missing values by providing a set of variables that define the combination of rows that should exist.

-Usually call complete() with names of existing variables, filling in the missing combinations. However, sometimes the individual variables are themselves incomplete, so you can instead provide your own data. 

-If the range of a variable is correct, but not all values are present, you could use full_seq(x, 1) to generate all values from min(x) to max(x) spaced out by 1.

### 18.3.3 Joins

-dplyr::anti_join(x, y) is a particularly useful tool here because it selects only the rows in x that don’t have a match in y. 

### 18.3.4 Exercises

Can you find any relationship between the carrier and the rows that appear to be missing from planes?

```{r}
missing_planes <- anti_join(flights, planes, by = "tailnum")

missing_planes |>
  group_by(carrier) |>
  summarize(missing_planes = n()) 
```
It appears that AA and MQ have the most missing rows.

## 18.4 Factors and empty groups

-A final type of missingness is the empty group, a group that doesn’t contain any observations, which can arise when working with factors.

-We can use .drop = FALSE to preserve all factor levels.

-All summary functions work with zero-length vectors, but they may return results that are surprising at first glance.

-Sometimes a simpler approach is to perform the summary and then make the implicit missings explicit with complete().


# 19  Joins

## 19.2 Keys

### 19.2.1 Primary and foreign keys

-A primary key is a variable or set of variables that uniquely identifies each observation. When more than one variable is needed, the key is called a compound key. 

-A foreign key is a variable (or set of variables) that corresponds to a primary key in another table.

### 19.2.2 Checking primary keys

-One way to do that is to count() the primary keys and look for entries where n is greater than one. 

-You should also check for missing values in your primary keys — if a value is missing then it can’t identify an observation!

### 19.2.3 Surrogate keys

-Surrogate keys can be particularly useful when communicating to other humans: 

### 19.2.4 Exercises

1. We forgot to draw the relationship between weather and airports in Figure 19.1. What is the relationship and how should it appear in the diagram?

-The column airports$faa is a foreign key of weather$origin.

2. weather only contains information for the three origin airports in NYC. If it contained weather records for all airports in the USA, what additional connection would it make to flights?

-We will be able to match the weather at the destinations as well.

3. The year, month, day, hour, and origin variables almost form a compound key for weather, but there’s one hour that has duplicate observations. Can you figure out what’s special about that hour?

-Because in the weather, there are two hours, one for dst, and the other presents the tzone.

4. We know that some days of the year are special and fewer people than usual fly on them (e.g., Christmas eve and Christmas day). How might you represent that data as a data frame? What would be the primary key? How would it connect to the existing data frames?

-We can create a new table special containing the pertaining information of the special dates. To match special with the exisiting tables, the keys would be year, month, and day.

5. Draw a diagram illustrating the connections between the Batting, People, and Salaries data frames in the Lahman package. Draw another diagram that shows the relationship between People, Managers, AwardsManagers. How would you characterize the relationship between the Batting, Pitching, and Fielding data frames?

```{r}
library(Lahman)
colnames(Lahman::Batting)
colnames(Lahman::Salaries)
colnames(Lahman::Managers)
colnames(Lahman::AwardsManagers)
```
-Batting contains batting statistics for players. The primary keys are playerID, yearID, stint, teamID, and lgID. The players’ biographical information are stored in Master and can be matched with playerID. The salary information for each player in each year can be matched with playerID, yearID, teamID, and lgID.

-Batting and Managers can be matched with playerID, yearID, teamID, and lgID. Mangers and AwardManagers are matched with playerID, yearID, teamID, and lgID.

-Batting, Pitching, and Fielding can be matched with playerID, yearID, stint, teamID, and lgID.

## 19.3 Basic Joins

### 19.3.1 Mutating joins

-A mutating join allows you to combine variables from two data frames: it first matches observations by their keys, then copies across variables from one data frame to the other. 

### 19.3.2 Specifying join keys

By default, left_join() will use all variables that appear in both data frames as the join key, the so called natural join.

join_by(tailnum) is short for join_by(tailnum == tailnum). It’s important to know about this fuller form for two reasons. Firstly, it describes the relationship between the two tables: the keys must be equal. That’s why this type of join is often called an equi join.

### 19.3.3 Filtering joins

 There are two types: semi-joins and anti-joins. Semi-joins keep all rows in x that have a match in y.
 
 Anti-joins are the opposite: they return all rows in x that don’t have a match in y.
 


### 19.3.4 Exercises

1.Find the 48 hours (over the course of the whole year) that have the worst delays Cross-reference it with the weather data. Can you see any patterns?
```{r, warning=FALSE}
library(nycflights13)
flights |>
  select(arr_delay, dep_delay, year:day, hour, origin) |>
  arrange(desc(dep_delay)) |>
  inner_join(weather, by = c(year = "year",
                             month = "month",
                             day = "day",
                             hour = "hour",
                             origin = "origin")) %>%
  ggplot(aes(x = wind_dir, y = precip, color = temp)) +
  geom_point(shape = "diamond")
```
-There should be no 48 hours delay as  the worst delay is 1,301 minutes which is about 21.7 hours. But the plot implies the worst delays related in the weather

2.Imagine you’ve found the top 10 most popular destinations using this code:
```{r}
flights2 <- flights |> 
  select(year, time_hour, origin, dest, tailnum, carrier)
top_dest <- flights2 |>
  count(dest, sort = TRUE) |>
  head(10)
```
How can you find all flights to those destinations?

```{r}
flights_to_top_dest <- flights2 |>
  filter(dest %in% top_dest$dest)
```

3. Does every departing flight have corresponding weather data for that hour?
```{r}
flights_weather <- flights |>
  left_join(weather, by = c("origin", "time_hour"))

sum(is.na(flights_weather$temp))
```
-There are 1573 flights that do not have corresponding weather data for that hour.

4. What do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.)
```{r}
flights_no_planes <- flights |>
  anti_join(planes, by = "tailnum")
flights_no_planes |>
  count(carrier, sort = TRUE)
```
-The result shows that AA (American Airlines) and MQ (Envoy Air) have the most flights without a matching record in planes, accounting for about 90% of the total flights without a matching record in planes. 

5. Add a column to planes that lists every carrier that has flown that plane. You might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned in previous chapters.

```{r}
flights_tailnum_carrier <- flights |>
  select(tailnum, carrier)

flights_tailnum_carrier <- flights_tailnum_carrier |>
  group_by(tailnum) |>
  summarise(carriers = paste(carrier, collapse = ", "))

planes_with_carriers <- planes |>
  left_join(flights_tailnum_carrier, by = "tailnum")

planes_with_carriers |>
  filter(str_detect(carriers, ",")) |>
  count()
```
-We reject the hypothesis since there 3177 planes having flown by more than one carrier.

6. Add the latitude and the longitude of the origin and destination airport to flights. Is it easier to rename the columns before or after the join?

```{r}
#Before


flights_with_origin <- flights |>
  left_join(airports, by = c("origin" = "faa"))


flights_with_origin_dest <- flights_with_origin |>
  left_join(airports, by = c("dest" = "faa"))

#After
flights_with_origin <- flights |>
  left_join(airports, by = c("origin" = "faa"))

flights_with_origin_dest <- flights_with_origin |>
  left_join(airports, by = c("dest" = "faa"))

flights_with_origin_dest
```

-It is easier to rename the columns before the join, because it avoids the confusion of the suffixes and makes the code more readable.

7. Compute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States:

airports |>
  semi_join(flights, join_by(faa == dest)) |>
  ggplot(aes(x = lon, y = lat)) +
    borders("state") +
    geom_point() +
    coord_quickmap()

```{r}
flights_avg_delay <- flights |>
  group_by(dest) |>
  summarise(avg_delay = mean(arr_delay, na.rm = TRUE))

flights_avg_delay_airports <- flights_avg_delay |>
  left_join(airports, by = c("dest" = "faa"))

#flights_avg_delay_airports |>
#  ggplot(aes(x = dest_lon, y = dest_lon)) +
#    borders("state") +
#    geom_point() +
#    coord_quickmap()
```

8. You might want to use the size or color of the points to display the average delay for each airport.
```{r, warning=FALSE}
flights_avg_delay_airports |>
  ggplot(aes(x = dest_lon, y = dest_lat)) +
    borders("state") +
    geom_point(aes(size = avg_delay)) +
    scale_size(range = c(1, 5)) +
    coord_quickmap()

flights_avg_delay_airports |>
  ggplot(aes(x = dest_lon, y = dest_lat)) +
    borders("state") +
    geom_point(aes(color = avg_delay)) +
    scale_color_gradient2(low = "green", mid = "yellow", high = "red") +
    coord_quickmap()

```
9. What happened on June 13 2013? Draw a map of the delays, and then use Google to cross-reference with the weather.
```{r}
flights_june_13 <- flights |>
  filter(year == 2013, month == 6)
```

## 19.4 How do joins work?

-A left join keeps all observations in x, Figure 19.5. Every row of x is preserved in the output because it can fall back to matching a row of NAs in y.

-A right join keeps all observations in y, Figure 19.6. Every row of y is preserved in the output because it can fall back to matching a row of NAs in x. The output still matches x as much as possible; any extra rows from y are added to the end.

-A full join keeps all observations that appear in x or y, Figure 19.7. Every row of x and y is included in the output because both x and y have a fall back row of NAs. Again, the output starts with all rows from x, followed by the remaining unmatched y rows.

### Row matching

-There are three possible outcomes for a row in x:

If it doesn’t match anything, it’s dropped.
If it matches 1 row in y, it’s preserved.
If it matches more than 1 row in y, it’s duplicated once for each match.

### 19.4.2 Filtering joins

-I strongly recommend watch the graph explain...

## 19.5 Non-equi joins (Please see the graph explain.)

1.Cross joins match every pair of rows.
2.Inequality joins use <, <=, >, and >= instead of ==.
3.Rolling joins are similar to inequality joins but only find the closest match.
4.Overlap joins are a special type of inequality join designed to work with ranges.

### 19.5.1 Cross joins

-Cross joins are useful when generating permutations. For example, the code below generates every possible pair of names. Since we’re joining df to itself, this is sometimes called a self-join. Cross joins use a different join function because there’s no distinction between inner/left/right/full when you’re matching every row.

### 19.5.2 Inequality joins

-Inequality joins are extremely general, so general that it’s hard to come up with meaningful specific use cases. One small useful technique is to use them to restrict the cross join so that instead of generating all permutations

### 19.5.3 Rolling joins

-Rolling joins are a special type of inequality join where instead of getting every row that satisfies the inequality. Thay are  useful when you have two tables of dates that don’t perfectly line up and you want to find (e.g.) the closest date in table 1 that comes before (or after) some date in table 2.

### 19.5.4 Overlap joins

-Overlap joins provide three helpers that use inequality joins to make it easier to work with intervals

### Exercises

1. Can you explain what’s happening with the keys in this equi join? Why are they different?

x |> full_join(y, join_by(key == key))
> # A tibble: 4 × 3
>     key val_x val_y
>   <dbl> <chr> <chr>
> 1     1 x1    y1   
> 2     2 x2    y2   
> 3     3 x3    <NA> 
> 4     4 <NA>  y3

x |> full_join(y, join_by(key == key), keep = TRUE)
> # A tibble: 4 × 4
>   key.x val_x key.y val_y
>   <dbl> <chr> <dbl> <chr>
> 1     1 x1        1 y1   
> 2     2 x2        2 y2   
> 3     3 x3       NA <NA> 
> 4    NA <NA>      4 y3

- The result of the join is a new table that contains all the columns from both tables, and only the rows that have the same value for key in both tables. In the example, the first join has keep = FALSE, so only one column named key is in the result. The second join has keep = TRUE, so there are two columns named key.x and key.y in the result.

2. When finding if any party period overlapped with another party period we used q < q in the join_by()? Why? What happens if you remove this inequality?

-The q < q condition in the join_by function is used to avoid self-joins, which are joins of a table with itself. If we want to find the overlapping party periods, we need to compare each row in the table with every other row, except itself. The q < q condition ensures that only rows with different values of q are joined, and avoids duplicate or redundant results.

#  Spreadsheets

## 20.2 Excel

```{r, warning=FALSE}
library(readxl)
library(tidyverse)
library(writexl)
```

### 20.2.2 Getting started

Most of readxl’s functions allow you to load Excel spreadsheets into R:

read_xls() reads Excel files with xls format.
read_xlsx() read Excel files with xlsx format.
read_excel() can read files with both xls and xlsx format. It guesses the file type based on the input.

### 20.2.3 Reading Excel spreadsheets

-read_excel() will read the file in as a tibble.

-You can specify which character strings should be recognized as NAs with the na argument

-The syntax is a bit different, though. Your options are "skip", "guess", "logical", "numeric", "date", "text" or "list".

-By specifying that age should be numeric, we have turned the one cell with the non-numeric entry (which had the value five) into an NA. In this case, we should read age in as "text" and then make the change once the data is loaded in R.

### 20.2.4 Reading worksheets

-You can read a single worksheet from a spreadsheet with the sheet argument in read_excel(). The default, which we’ve been relying on up until now, is the first sheet.

-Alternatively, you can use excel_sheets() to get information on all worksheets in an Excel spreadsheet, and then read the one(s) you’re interested in.

-Once you know the names of the worksheets, you can read them in individually with read_excel().

-We can put worksheet together with bind_rows().

### 20.2.5 Reading part of a sheet

-Since many use Excel spreadsheets for presentation as well as for data storage, it’s quite common to find cell entries in a spreadsheet that are not part of the data you want to read into R.

- You can use the readxl_example() function to locate the spreadsheet on your system in the directory where the package is installed. This function returns the path to the spreadsheet, which you can use in read_excel() as usual.

### 20.2.6 Data types

-A cell can be one of four things:

-A boolean, like TRUE, FALSE, or NA.

-A number, like “10” or “10.5”.

-A datetime, which can also include time like “11/1/21” or “11/1/21 3:00 PM”.

-A text string, like “ten”.

-When working with spreadsheet data, it’s important to keep in mind that the underlying data can be very different than what you see in the cell. 

### 20.2.7 Writing to Excel

```{r}
bake_sale <- tibble(
  item     = factor(c("brownie", "cupcake", "cookie")),
  quantity = c(10, 5, 8)
)

bake_sale

write_xlsx(bake_sale, path = "bake-sale.xlsx")

read_excel("bake-sale.xlsx")
```

### 20.2.8 Formatted output

-The writexl package is a light-weight solution for writing a simple Excel spreadsheet, but if you’re interested in additional features like writing to sheets within a spreadsheet and styling, you will want to use the openxlsx package. 

### 20.2.9 Exercises (X in Q3)

1. In an Excel file, create the following dataset and save it as survey.xlsx. Alternatively, you can download it as an Excel file from here.

A spreadsheet with 3 columns (group, subgroup, and id) and 12 rows. The group column has two values: 1 (spanning 7 merged rows) and 2 (spanning 5 merged rows). The subgroup column has four values: A (spanning 3 merged rows), B (spanning 4 merged rows), A (spanning 2 merged rows), and B (spanning 3 merged rows). The id column has twelve values, numbers 1 through 12.

Then, read it into R, with survey_id as a character variable and n_pets as a numerical variable.

```{r}
library(readxl)
survey <- read_excel("C:/Users/He Zhefeng/Desktop/CA/survey.xlsx", col_types = c("text", "numeric"))
survey
```


2. In another Excel file, create the following dataset and save it as roster.xlsx. Alternatively, you can download it as an Excel file from here.

A spreadsheet with 3 columns (group, subgroup, and id) and 12 rows. The group column has two values: 1 (spanning 7 merged rows) and 2 (spanning 5 merged rows). The subgroup column has four values: A (spanning 3 merged rows), B (spanning 4 merged rows), A (spanning 2 merged rows), and B (spanning 3 merged rows). The id column has twelve values, numbers 1 through 12.

Then, read it into R. The resulting data frame should be called roster and should look like the following.

```{r}
roster <- read_excel("C:/Users/He Zhefeng/Downloads/roster.xlsx")
roster
```


In a new Excel file, create the following dataset and save it as sales.xlsx. Alternatively, you can download it as an Excel file from here.

A spreadsheet with 2 columns and 13 rows. The first two rows have text containing information about the sheet. Row 1 says "This file contains information on sales". Row 2 says "Data are organized by brand name, and for each brand, we have the ID number for the item sold, and how many are sold.". Then there are two empty rows, and then 9 rows of data.

a. Read sales.xlsx in and save as sales. The data frame should look like the following, with id and n as column names and with 9 rows.

> # A tibble: 9 × 2
>   id      n    
>   <chr>   <chr>
> 1 Brand 1 n    
> 2 1234    8    
> 3 8721    2    
> 4 1822    3    
> 5 Brand 2 n    
> 6 3333    1    
> 7 2156    3    
> 8 3987    6    
> 9 3216    5
b. Modify sales further to get it into the following tidy format with three columns (brand, id, and n) and 7 rows of data. Note that id and n are numeric, brand is a character variable.

> # A tibble: 7 × 3
>   brand      id     n
>   <chr>   <dbl> <dbl>
> 1 Brand 1  1234     8
> 2 Brand 1  8721     2
> 3 Brand 1  1822     3
> 4 Brand 2  3333     1
> 5 Brand 2  2156     3
> 6 Brand 2  3987     6
> 7 Brand 2  3216     5

```{r}
library(tidyverse)
# library(stringr)
library(readxl)
sales <- read_excel("C:/Users/He Zhefeng/Downloads/sales.xlsx", skip = 4, col_types = c("text", "numeric"))
sales <- read_excel("sales.xlsx", skip = 3, col_names = c("id", "n"), col_types = c("text", "numeric"))

sales = sales |>
  mutate(brand = if_else(is.na(n),id,NA_character_)) %>%
  fill(brand) %>%
  filter(!is.na(n)) %>%
  relocate(brand)
  
sum(is.na(sales$n))  
sales_brand = sales %>%
  filter(brand == "Brand 1")
nrow(sales_brand)


```
4.Recreate the bake_sale data frame, write it out to an Excel file using the write.xlsx() function from the openxlsx package.
```{r,warning=FALSE}
library(tibble)
bake_sale <- tribble(
  ~item, ~price, ~quantity,
  "cake", 2, 10,
  "cookie", 1, 25,
  "pie", 3, 8,
  "muffin", 2, 15)
bake_sale
library(openxlsx)
write.xlsx(bake_sale, "bake_sale.xlsx")
```

5.In Chapter 7 you learned about the janitor::clean_names() function to turn column names into snake case. Read the students.xlsx file that we introduced earlier in this section and use this function to “clean” the column names.
```{r}
library(janitor)
students <- read_csv("https://pos.it/r4ds-students-csv") |>
  clean_names()
```

6.What happens if you try to read in a file with .xlsx extension with read_xls()?

-We will get an error message saying that the file is not a valid xls file, as read_xls() can only read files in the older xls format

## 20.3 Google Sheets

### 20.3.1 Prerequisites
```{r}
library(googlesheets4)
library(tidyverse)
```

### 20.3.2 Getting started

-The main function of the googlesheets4 package is read_sheet(), which reads a Google Sheet from a URL or a file id. This function also goes by the name range_read().

-We can also create a brand new sheet with gs4_create() or write to an existing sheet with sheet_write() and friends.

### 20.3.3 Reading Google Sheets

-The first argument to read_sheet() is the URL of the file to read, and it returns a tibble:
https://docs.google.com/spreadsheets/d/1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w.

-Just like we did with read_excel(), we can supply column names, NA strings, and column types to read_sheet().

-You can obtain a list of all sheets within a Google Sheet with sheet_names():

-Finally, just like with read_excel(), we can read in a portion of a Google Sheet by defining a range in read_sheet(). Note that we’re also using the gs4_example() function below to locate an example Google Sheet that comes with the googlesheets4 package.

### 20.3.4 Writing to Google Sheets

-You can write from R to Google Sheets with write_sheet(). The first argument is the data frame to write, and the second argument is the name (or other identifier) of the Google Sheet to write to

### 20.3.5 Authentication

-While you can read from a public Google Sheet without authenticating with your Google account and with gs4_deauth(), reading a private sheet or writing to a sheet requires authentication so that googlesheets4 can view and manage your Google Sheets.

### 20.3.6 Exercises

1.Read the students dataset from earlier in the chapter from Excel and also from Google Sheets, with no additional arguments supplied to the read_excel() and read_sheet() functions. Are the resulting data frames in R exactly the same? If not, how are they different?
```{r, warning=FALSE}
studentsE <- read_csv("https://pos.it/r4ds-students-csv")
# studentsG <- read_sheet("https://pos.it/r4ds-students-csv")

```

2.Read the Google Sheet titled survey from https://pos.it/r4ds-survey, with survey_id as a character variable and n_pets as a numerical variable.
```{r}
#survey <- read_sheet("https://pos.it/r4ds-survey", sheet = "survey")
```


3.Read the Google Sheet titled roster from https://pos.it/r4ds-roster. The resulting data frame should be called roster and should look like the following.

> # A tibble: 12 × 3
>    group subgroup    id
#>    <dbl> <chr>    <dbl>
#>  1     1 A            1
#>  2     1 A            2
#>  3     1 A            3
#>  4     1 B            4
#>  5     1 B            5
#>  6     1 B            6
#>  7     1 B            7
#>  8     2 A            8
#>  9     2 A            9
#> 10     2 B           10
#> 11     2 B           11
#> 12     2 B           12

# 21  Databases

## 21.1.1 Prerequisites

```{r}
library(DBI)
library(dbplyr)
library(tidyverse)
```

## 21.2 Database basics

-Database tables are stored on disk and can be arbitrarily large. Data frames are stored in memory, and are fundamentally limited (although that limit is still plenty large for many problems).

-Database tables almost always have indexes. Much like the index of a book, a database index makes it possible to quickly find rows of interest without having to look at every single row. Data frames and tibbles don’t have indexes, but data.tables do, which is one of the reasons that they’re so fast.

-Most classical databases are optimized for rapidly collecting data, not analyzing existing data. These databases are called row-oriented because the data is stored row-by-row, rather than column-by-column like R. More recently, there’s been much development of column-oriented databases that make analyzing the existing data much faster.

-Client-server DBMS’s run on a powerful central server, which you connect from your computer (the client). They are great for sharing data with multiple people in an organization. Popular client-server DBMS’s include PostgreSQL, MariaDB, SQL Server, and Oracle.

-Cloud DBMS’s, like Snowflake, Amazon’s RedShift, and Google’s BigQuery, are similar to client server DBMS’s, but they run in the cloud. This means that they can easily handle extremely large datasets and can automatically provide more compute resources as needed.

-In-process DBMS’s, like SQLite or duckdb, run entirely on your computer. They’re great for working with large datasets where you’re the primary user.

## 21.3 Connecting to a database

-You’ll always use DBI (database interface) because it provides a set of generic functions that connect to the database, upload data, run SQL queries, etc.

-You’ll also use a package tailored for the DBMS you’re connecting to. This package translates the generic DBI commands into the specifics needed for a given DBMS. There’s usually one package for each DBMS, e.g. RPostgres for PostgreSQL and RMariaDB for MySQL.

-Concretely, you create a database connection using DBI::dbConnect(). The first argument selects the DBMS2, then the second and subsequent arguments describe how to connect to it (i.e. where it lives and the credentials that you need to access it).

## 21.3.1 In this book

```{r}
con <- DBI::dbConnect(duckdb::duckdb())
con <- DBI::dbConnect(duckdb::duckdb(), dbdir = "duckdb")
```

## 21.3.2 Load some data

```{r}
dbWriteTable(con, "mpg", ggplot2::mpg)
dbWriteTable(con, "diamonds", ggplot2::diamonds)
```

## 21.3.3 DBI basics

-You can check that the data is loaded correctly by using a couple of other DBI functions: dbListTables() lists all tables in the database3 and dbReadTable() retrieves the contents of a table.

```{r}
dbListTables(con)

con |> 
  dbReadTable("diamonds") |> 
  as_tibble()

# you can use dbGetQuery() to get the results of running a query on the database:
sql <- "
  SELECT carat, cut, clarity, color, price 
  FROM diamonds 
  WHERE price > 15000
"
as_tibble(dbGetQuery(con, sql))
```

## 21.4 dbplyr basics

-dbplyr is a dplyr backend, which means that you keep writing dplyr code but the backend executes it differently. In this, dbplyr translates to SQL; other backends include dtplyr which translates to data.table, and multidplyr which executes your code on multiple cores.

-This object is lazy; when you use dplyr verbs on it, dplyr doesn’t do any work: it just records the sequence of operations that you want to perform and only performs them when needed. 

-You can see the SQL code generated by the dplyr function show_query().

-To get all the data back into R, you call collect(). Behind the scenes, this generates the SQL, calls dbGetQuery() to get the data, then turns the result into a tibble

## 21.5 SQL

### 21.5.1 SQL basics

-The top-level components of SQL are called statements. Common statements include CREATE for defining new tables, INSERT for adding data, and SELECT for retrieving data.

-A query is made up of clauses. There are five important clauses: SELECT, FROM, WHERE, ORDER BY, and GROUP BY. Every query must have the SELECT4 and FROM5 clauses and the simplest query is SELECT * FROM table, which selects all columns from the specified table . 

-WHERE and ORDER BY control which rows are included and how they are ordered

-GROUP BY converts the query to a summary, causing aggregation to happen

-Two important differences between dplyr verbs and SELECT clauses:

-1. In SQL, case doesn’t matter: you can write select, SELECT, or even SeLeCt. In this book we’ll stick with the common convention of writing SQL keywords in uppercase to distinguish them from table or variables names.

-2. In SQL, order matters: you must always write the clauses in the order SELECT, FROM, WHERE, GROUP BY, ORDER BY. Confusingly, this order doesn’t match how the clauses actually evaluated which is first FROM, then WHERE, GROUP BY, SELECT, and ORDER BY.

### 21.5.2 SELECT

-The SELECT clause is the workhorse of queries and performs the same job as select(), mutate(), rename(), relocate(), and, as you’ll learn in the next section, summarize()

-select(), rename(), and relocate() have very direct translations to SELECT as they just affect where a column appears (if at all) along with its name

-The translations for mutate() are similarly straightforward: each variable becomes a new expression in SELECT

### 21.5.3 FROM

-The FROM clause defines the data source. It’s going to be rather uninteresting for a little while, because we’re just using single tables. 

### 21.5.4 GROUP BY

-group_by() is translated to the GROUP BY6 clause and summarize() is translated to the SELECT clause

### 21.5.5 WHERE

-filter() is translated to the WHERE clause

-A few important details to note:

1. | becomes OR and & becomes AND.

2. SQL uses = for comparison, not ==. SQL doesn’t have assignment, so there’s no potential for confusion there.

3. SQL uses only '' for strings, not "". In SQL, "" is used to identify variables, like R’s ``.

-Another useful SQL operator is IN, which is very close to R’s %in%

-Note that if you filter() a variable that you created using a summarize, dbplyr will generate a HAVING clause, rather than a WHERE clause. This is a one of the idiosyncrasies of SQL: WHERE is evaluated before SELECT and GROUP BY, so SQL needs another clause that’s evaluated afterwards.

### 21.5.6 ORDER BY

-Ordering rows involves a straightforward translation from arrange() to the ORDER BY clause

### 21.5.7 Subqueries

-A subquery is just a query used as a data source in the FROM clause, instead of the usual table

-dbplyr typically uses subqueries to work around limitations of SQL. For example, expressions in the SELECT clause can’t refer to columns that were just created. That means that the following (silly)

### 21.5.8 Joins

-dplyr’s names for these functions are so closely connected to SQL that you can easily guess the equivalent SQL for inner_join(), right_join(), and full_join()

### 21.5.9 Other verbs

-dbplyr also translates other verbs like distinct(), slice_*(), and intersect(), and a growing selection of tidyr functions like pivot_longer() and pivot_wider(). 


### 21.5.10 Exercises

1. What is distinct() translated to? How about head()?

-The distinct() function in SQL is used to remove duplicate values from a result set. The head() function in SQL is not a standard function, but some databases may have a similar function to return the first n rows of a result set.

2. Explain what each of the following SQL queries do and try recreate them using dbplyr.

SELECT * 
FROM flights
WHERE dep_delay < arr_delay

SELECT *, distance / (air_time / 60) AS speed
FROM flights

-The first part of 'select' picks up flights where the delayed departure is shorter than delayed arrive.

-The second part of 'select' chooses the flight speed calculated in hours.

## 21.6 Function translations

-In SQL, the GROUP BY clause is used exclusively for summaries so here you can see that the grouping has moved from the PARTITION BY argument to OVER

-Here it’s important to arrange() the data, because SQL tables have no intrinsic order. In fact, if you don’t use arrange() you might get the rows back in a different order every time! Notice for window functions, the ordering information is repeated: the ORDER BY clause of the main query doesn’t automatically apply to window functions.

-Another important SQL function is CASE WHEN. It’s used as the translation of if_else() and case_when(), the dplyr function that it directly inspired.

-Another important SQL function is CASE WHEN. It’s used as the translation of if_else() and case_when(), the dplyr function that it directly inspired.

# 22  Arrow

## 22.1.1 Prerequisites

```{r, warning=FALSE}
library(tidyverse)
library(arrow)
```

## 22.2 Getting the data

-The data is a 9GB CSV file, available online at data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6...

## 22.3 Opening a dataset

-At 9 GB, this file is large enough that we probably don’t want to load the whole thing into memory. A good rule of thumb is that you usually want at least twice as much memory as the size of the data, and many laptops top out at 16 GB. This means we want to avoid read_csv() and instead use the arrow::open_dataset()

-open_dataset() will scan a few thousand rows to figure out the structure of the dataset.

-Once the data has been scanned by open_dataset(), it records what it’s found and stops; it will only read further rows as you specifically request them.

## 22.4 The parquet format

### 22.4.1 Advantages of parquet

-Like CSV, parquet is used for rectangular data, but instead of being a text format that you can read with any file editor, it’s a custom binary format designed specifically for the needs of big data with FOUR features:

-1. Parquet files are usually smaller than the equivalent CSV file. Parquet relies on efficient encodings to keep file size down, and supports file compression. This helps make parquet files fast because there’s less data to move from disk to memory.

-2. Parquet files have a rich type system. As we talked about in Section 7.3, a CSV file does not provide any information about column types. For example, a CSV reader has to guess whether "08-10-2022" should be parsed as a string or a date. In contrast, parquet files store data in a way that records the type along with the data.

-3. Parquet files are “column-oriented”. This means that they’re organized column-by-column, much like R’s data frame. This typically leads to better performance for data analysis tasks compared to CSV files, which are organized row-by-row.

-4. Parquet files are “chunked”, which makes it possible to work on different parts of the file at the same time, and, if you’re lucky, to skip some chunks altogether.


### 22.4.2 Partitioning

-As a rough guide, arrow suggests that you avoid files smaller than 20MB and larger than 2GB and avoid partitions that produce more than 10,000 files.

### 22.4.3 Rewriting the Seattle library data

-Use parquet to rewrite single 9GB CSV file has been rewritten into 18 parquet files. 

```{r}
#pq_path <- "data/seattle-library-checkouts"

#seattle_csv |>
##  write_dataset(path = pq_path, format = "parquet")

#tibble(
#  files = list.files(pq_path, recursive = TRUE),
#  size_MB = file.size(file.path(pq_path, files)) / 1024^2
#)
```

## 22.5 Using dplyr with arrow

- The ~100x speedup in performance is attributable to two factors: the multi-file partitioning, and the format of individual files:

- Partitioning improves performance because this query uses CheckoutYear == 2021 to filter the data, and arrow is smart enough to recognize that it only needs to read 1 of the 18 parquet files.


- The parquet format improves performance by storing data in a binary format that can be read more directly into memory. The column-wise format and rich metadata means that arrow only needs to read the four columns actually used in the query (CheckoutYear, MaterialType, CheckoutMonth, and Checkouts).

### 22.5.2 Using duckdb with arrow

-There’s one last advantage of parquet and arrow — it’s very easy to turn an arrow dataset into a DuckDB database (Chapter 21) by calling arrow::to_duckdb()

-The neat thing about to_duckdb() is that the transfer doesn’t involve any memory copying, and speaks to the goals of the arrow ecosystem: enabling seamless transitions from one computing environment to another.


# 23 Hierarchical data

## 23.1 Introduction
```{r}
library(tidyverse)
library(repurrrsive)
library(jsonlite)
```
## 23.2 Lists

-If you want to store elements of different types in the same vector, you’ll need a list, which you create with list()

-A useful alternative is str(), which generates a compact display of the structure, de-emphasizing the contents

-str() displays each child of the list on its own line. It displays the name, if present, then an abbreviation of the type, then the first few values.

### 23.2.1 Hierarchy

- c() generates a flat vector

- As lists get more complex, str() gets more useful, as it lets you see the hierarchy at a glance

- As lists get even larger and more complex, str() eventually starts to fail, and you’ll need to switch to View()

### 23.2.2 List-columns

-Lists can also live inside a tibble, where we call them list-columns. List-columns are useful because they allow you to place objects in a tibble that wouldn’t usually belong in there. In particular, list-columns are used a lot in the tidymodels ecosystem, because they allow you to store things like model outputs or resamples in a data frame.

-The default print method just displays a rough summary of the contents. The list column could be arbitrarily complex, so there’s no good way to print it.

## 23.3 Unnesting

-List-columns tend to come in two basic forms: named and unnamed. When the children are named, they tend to have the same names in every row. 

-When the children are unnamed, the number of elements tends to vary from row-to-row. For example, in df2, the elements of list-column y are unnamed and vary in length from one to three.

### 23.3.1 unnest_wider() 

-When each row has the same number of elements with the same names, like df1, it’s natural to put each component into its own column with unnest_wider()

-By default, the names of the new columns come exclusively from the names of the list elements, but you can use the names_sep argument to request that they combine the column name and the element name. 

### 23.3.2 unnest_longer()

-When each row contains an unnamed list, it’s most natural to put each element into its own row with unnest_longer()

-Note how x is duplicated for each element inside of y: we get one row of output for each element inside the list-column.

-If you want to preserve that row, adding NA in y, set keep_empty = TRUE.

### 23.3.3 Inconsistent types

-Because unnest_longer() can’t find a common type of vector, it keeps the original types in a list-column.

### EXE

1. When each row has the same number of elements with the same names, like df1, it’s natural to put each component into its own column with unnest_wider():

```{r}

df2 <- tribble(
  ~x, ~y,
  1, list(11, 12, 13),
  2, list(21),
  3, list(31, 32),
)
df2 |> unnest_wider(y, names_sep = "_")
```


2.What happens when you use unnest_longer() with named list-columns like df1? What additional information do you get in the output? How can you suppress that extra detail?

```{r}
df1 <- tribble(
  ~x, ~y,
  1, list(a = 11, b = 12),
  2, list(a = 21, b = 22),
  3, list(a = 31, b = 32),
)

df1 |> unnest_longer(y)
df1 |> unnest_longer(y, indices_include = FALSE)
```
When use unnest_longer() with named list-columns, we get an additional column that stores the inner names of the values.

3.From time-to-time you encounter data frames with multiple list-columns with aligned values. For example, in the following data frame, the values of y and z are aligned (i.e. y and z will always have the same length within a row, and the first value of y corresponds to the first value of z). What happens if you apply two unnest_longer() calls to this data frame? How can you preserve the relationship between x and y? (Hint: carefully read the docs).
```{r}
df4 <- tribble(
  ~x, ~y, ~z,
  "a", list("y-a-1", "y-a-2"), list("z-a-1", "z-a-2"),
  "b", list("y-b-1", "y-b-2", "y-b-3"), list("z-b-1", "z-b-2", "z-b-3")
)
df4 |>
  unnest_longer(y) |>
  unnest_longer(z)

df4 |>
  unnest_wider(c(y, z), names_sep = "_")

```
-In this case, we can use unnest_longer() to expand each list-column into rows. However, this will create duplicate rows for the other columns, and we will lose the original order of the values. To preserve the relationship between x and y, we can use unnest_wider() first to create a nested data frame for each pair of y and z, and then use unnest_longer() to expand the nested data frame. 

## 23.4 Case studies

### 23.4.1 Very wide data

-start with gh_repos. This is a list that contains data about a collection of GitHub repositories retrieved using the GitHub API

-gh_repos is a list

-At first glance, it might seem like we haven’t improved the situation: while we have more rows (176 instead of 6) each element of json is still a list. However, there’s an important difference: now each element is a named list so we can use unnest_wider() to put each element into its own column


```{r}
repos <- tibble(json = gh_repos)
repos
```
### EXE (*)
1.Roughly estimate when gh_repos was created. Why can you only roughly estimate the date?

-Because we do not know when the repurrrsive package was updated with the new data, or if the data was collected at different points in time. Therefore, we can only roughly estimate the date based on the available information.

2.The owner column of gh_repo contains a lot of duplicated information because each owner can have many repos. Can you construct an owners data frame that contains one row for each owner? (Hint: does distinct() work with list-cols?)

```{r}
owners <- repos |>
  distinct(repos$owner, .keep_all = TRUE) |>
  select(owner = repos$owner)

owners
```
3. Follow the steps used for titles to create similar tables for the aliases, allegiances, books, and TV series for the Game of Thrones characters.

```{r}
got_df <- tibble(
  character = got_chars,
  aliases = map(character, "aliases"),
  allegiances = map(character, "allegiances"),
  books = map(character, "books"),
  tvSeries = map(character, "tvSeries")
)

got_aliases <- got_df |>
  unnest_longer(aliases, values_to = "alias", indices_to = "alias_id")

got_tvSeries <- got_df |>
  unnest_longer(tvSeries, values_to = "tvSeries", indices_to = "tvSeries_id")

got_aliases

got_tvSeries

```

4.Explain the following code line-by-line. Why is it interesting? Why does it work for got_chars but might not work in general?
```{r}
tibble(json = got_chars) |> 
  unnest_wider(json) |> 
  select(id, where(is.list)) |> 
  pivot_longer(
    where(is.list), 
    names_to = "name", 
    values_to = "value"
  ) |>  
  unnest_longer(value)
```

-The first line creates a tibble with one column called json, which contains the got_chars list. The pipe operator (|>) passes the tibble to the next function.

-The second line uses the unnest_wider() function to turn each element of the json list-column into a column, using the names of the elements as the column names. This produces a tibble with 30 columns, one for each attribute of the characters. The pipe operator passes the tibble to the next function.

-The third line uses the select() function to keep only the columns that are list-columns, which are the ones that contain nested information such as aliases, allegiances, books, etc. It also keeps the id column, which is a unique identifier for each character. This produces a tibble with 11 columns, one for the id and 10 for the list-columns. The pipe operator passes the tibble to the next function.

-The fourth to seventh lines use the pivot_longer() function to turn the tibble into a long format, where each list-column becomes a row. The names_to argument specifies the name for the new column that contains the original column names, and the values_to argument specifies the name for the new column that contains the list values.

5. In gmaps_cities, what does address_components contain? Why does the length vary between rows? Unnest it appropriately to figure it out. (Hint: types always appears to contain two elements. Does unnest_wider() make it easier to work with than unnest_longer()?) .

```{r}
#gmaps_cities |>
#  unnest_wider(address_components, names_repair = "unique")
```
-The address_components list-column in gmaps_cities contains a list of subcomponents of the formatted_address for each city, such as street number, route, locality, administrative area, country, and postal code. Each subcomponent has a long_name, a short_name, and a list of types that indicate the level of the address hierarchy. 

-The length of the address_components list varies between rows because different cities may have different levels of address hierarchy, depending on the country and the region.

-To unnest the address_components list-column appropriately, we can use the unnest_wider() function from the tidyr package, which will turn each element of the list into a column, using the names of the elements as the column names.

## 23.5 JSON

### 23.5.1 Data types

-The simplest type is a null (null) which plays the same role as NA in R. It represents the absence of data.

-A string is much like a string in R, but must always use double quotes.

-A number is similar to R’s numbers: they can use integer (e.g., 123), decimal (e.g., 123.45), or scientific (e.g., 1.23e3) notation. JSON doesn’t support Inf, -Inf, or NaN.

-A boolean is similar to R’s TRUE and FALSE, but uses lowercase true and false.

-Both arrays and objects are similar to lists in R; the difference is whether or not they’re named. An array is like an unnamed list, and is written with []. For example [1, 2, 3] is an array containing 3 numbers, and [null, 1, "string", false] is an array that contains a null, a number, a string, and a boolean. An object is like a named list, and is written with {}. The names (keys in JSON terminology) are strings, so must be surrounded by quotes. 

### 23.5.2 jsonlite

-To convert JSON into R data structures, we recommend the jsonlite package, by Jeroen Ooms. We’ll use only two jsonlite functions: read_json() and parse_json(). In real life, you’ll use read_json() to read a JSON file from disk.

-jsonlite has another important function called fromJSON(). We don’t use it here because it performs automatic simplification (simplifyVector = TRUE).

### 23.5.3 Starting the rectangling process


-In most cases, JSON files contain a single top-level array, because they’re designed to provide data about multiple “things”, e.g., multiple pages, or multiple records, or multiple results. 

-In rarer cases, the JSON file consists of a single top-level JSON object, representing one “thing”.

### 23.5.4 Exercises

1. Rectangle the df_col and df_row below. They represent the two ways of encoding a data frame in JSON.

```{r}
json_col <- parse_json('
  {
    "x": ["a", "x", "z"],
    "y": [10, null, 3]
  }
')
json_row <- parse_json('
  [
    {"x": "a", "y": 10},
    {"x": "x", "y": null},
    {"x": "z", "y": 3}
  ]
')

df_col <- tibble(json = list(json_col)) 
df_row <- tibble(json = json_row)
```

```{r}
df_col |>
  unnest_wider(json)

df_row |>
  unnest_wider(json)
```

# 24  Web scraping

## 24.1.1 Prerequisites
```{r}
library(tidyverse)
library(rvest)
```

## 24.2 Scraping ethics and legalities

### 24.2.1 Terms of service

-US courts have generally found that simply putting the terms of service in the footer of the website isn’t sufficient for you to be bound by them, e.g., HiQ Labs v. LinkedIn. Generally, to be bound to the terms of service, you must have taken some explicit action like creating an account or checking a box. 

### 24.2.2 Personally identifiable information

-Even if the data is public, you should be extremely careful about scraping personally identifiable information like names, email addresses, phone numbers, dates of birth, etc. Europe has particularly strict laws about the collection or storage of such data (GDPR), and regardless of where you live you’re likely to be entering an ethical quagmire. 


### 24.2.3 Copyright

-Finally, you also need to worry about copyright law. Copyright law is complicated, but it’s worth taking a look at the US law which describes exactly what’s protected: “[…] original works of authorship fixed in any tangible medium of expression, […]”. It then goes on to describe specific categories that it applies like literary works, musical works, motion pictures and more. Notably absent from copyright protection are data. This means that as long as you limit your scraping to facts, copyright protection does not apply. (But note that Europe has a separate “sui generis” right that protects databases.)

## 24.3 HTML basics

-HTML stands for HyperText Markup Language.

-HTML has a hierarchical structure formed by elements which consist of a start tag (e.g., <tag>), optional attributes (id='first'), an end tag4 (like </tag>), and contents (everything in between the start and end tag).


### 24.3.1 Elements

-Every HTML page must be in an <html> element, and it must have two children: <head>, which contains document metadata like the page title, and <body>, which contains the content you see in the browser.

-Block tags like <h1> (heading 1), <section> (section), <p> (paragraph), and <ol> (ordered list) form the overall structure of the page.

-Inline tags like <b> (bold), <i> (italics), and <a> (link) format text inside block tags.


### Attributes

-Tags can have named attributes which look like name1='value1' name2='value2'. Two of the most important attributes are id and class, which are used in conjunction with CSS (Cascading Style Sheets) to control the visual appearance of the page. These are often useful when scraping data off a page. Attributes are also used to record the destination of links (the href attribute of <a> elements) and the source of images (the src attribute of the <img> element).

## 24.4 Extracting data

-To get started scraping, you’ll need the URL of the page you want to scrape, which you can usually copy from your web browser. 

-rvest also includes a function that lets you write HTML inline. We’ll use this a bunch in this chapter as we teach how the various rvest functions work with simple examples.

```{r}
library(tidyverse)
library(rvest)

html <- read_html("http://rvest.tidyverse.org/")
html

html <- minimal_html("
  <p>This is a paragraph</p>
  <ul>
    <li>This is a bulleted list</li>
  </ul>
")
html
```
### 24.4.1 Find elements

-We’ll come back to CSS selectors in more detail in Section 24.5, but luckily you can get a long way with just three:

-p selects all <p> elements.

-.title selects all elements with class “title”.

-#title selects the element with the id attribute that equals “title”. Id attributes must be unique within a document, so this will only ever select a single element.

-Use html_elements() to find all elements that match the selector:
```{r}
html |> html_elements("p")

html |> html_elements(".important")

html |> html_elements("#first")
```

-Another important function is html_element() which always returns the same number of outputs as inputs.

```{r}
html |> html_element("p")
```
-There’s an important difference between html_element() and html_elements() when you use a selector that doesn’t match any elements. html_elements() returns a vector of length 0, where html_element() returns a missing value.

### 24.4.2 Nesting selections

-In most cases, you’ll use html_elements() and html_element() together, typically using html_elements() to identify elements that will become observations then using html_element() to find elements that will become variables. 

-The distinction between html_element() and html_elements() isn’t important for name, but it is important for weight. We want to get one weight for each character, even if there’s no weight <span>.

### 24.4.3 Text and attributes

-html_text2()6 extracts the plain text contents of an HTML element

-html_attr() extracts data from attributes

-html_attr() always returns a string, so if you’re extracting numbers or dates, you’ll need to do some post-processing.

### 24.4.4 Tables

-HTML tables are built up from four main elements: <table>, <tr> (table row), <th> (table heading), and <td> (table data). 

-rvest provides a function that knows how to read this sort of data: html_table(). It returns a list containing one tibble for each table found on the page. Use html_element() to identify the table you want to extract

-Note that x and y have automatically been converted to numbers. This automatic conversion doesn’t always work, so in more complex scenarios you may want to turn it off with convert = FALSE and then do your own conversion.

## 24.5 Finding the right selectors

-Figuring out the selector you need for your data is typically the hardest part of the problem. You’ll often need to do some experimenting to find a selector that is both specific (i.e. it doesn’t select things you don’t care about) and sensitive (i.e. it does select everything you care about). 

-SelectorGadget is a javascript bookmarklet that automatically generates CSS selectors based on the positive and negative examples that you provide. 

-Every modern browser comes with some toolkit for developers, but we recommend Chrome, even if it isn’t your regular browser: its web developer tools are some of the best and they’re immediately available. Right click on an element on the page and click Inspect. This will open an expandable view of the complete HTML page, centered on the element that you just clicked. You can use this to explore the page and get a sense of what selectors might work.

## 24.6 Putting it all together

### 24.6.1 StarWars

-rvest includes a very simple example in vignette("starwars"). This is a simple page with minimal HTML so it’s a good place to start.

-This retrieves seven elements matching the seven movies found on that page, suggesting that using section as a selector is good. Extracting the individual elements is straightforward since the data is always found in the text.

## 24.7 Dynamic sites

-So far we have focused on websites where html_elements() returns what you see in the browser and discussed how to parse what it returns and how to organize that information in tidy data frames. From time-to-time, however, you’ll hit a site where html_elements() and friends don’t return anything like what you see in the browser. In many cases, that’s because you’re trying to scrape a website that dynamically generates the content of the page with javascript.

-It’s still possible to scrape these types of sites, but rvest needs to use a more expensive process: fully simulating the web browser including running all javascript. This functionality is not available at the time of writing, but it’s something we’re actively working on and might be available by the time you read this. 

## 24 XML special code challenge! 

Can you retrieve this XML file: 
https://stats.oecd.org/restsdmx/sdmx.ashx/GetDataStructure/FAMILY

Then, extract from the file, the list of codes and definitions from 
<CodeList id="CL_FAMILY_IND" agencyID="OECD">
<Name xml:lang="en">Indicator</Name>
<Name xml:lang="fr">Indicateur</Name>
<Code value="D1">

New tibble: 

variable    definition    fr_definition
FAM1    Total fertility rate Taux de fécondité
FAM2    Mean age of women at childbirth   Âge moyen des femmes à la naissance de l'enfant

```{r}

library(xml2)


# Define the URL of the XML file
url <- "https://stats.oecd.org/restsdmx/sdmx.ashx/GetDataStructure/FAMILY"

# Read the XML file from the URL and parse it into a document object
doc <- read_xml(url)

# Define the namespace prefix and URI for the XML file. (This step is missed in the first try, and resulting in empty lists of codelists)
ns <- c(s = "http://www.SDMX.org/resources/SDMXML/schemas/v2_0/structure")

# Find the CodeList element with id="CL_FAMILY_IND" using XPath with the namespace prefix
codelist <- xml_find_one(doc, "//s:CodeList[@id='CL_FAMILY_IND']", ns = ns)

# Find all the Code elements within the CodeList element
codes <- xml_find_all(codelist, ".//s:Code", ns = ns)

# Extract the value attribute and the definition from each Code element (En and fr)
value <- xml_attr(codes, "value")
definition <- xml_text(xml_find_first(codes, ".//s:Description[@xml:lang='en']", ns = ns))
fr_definition <- xml_text(xml_find_first(codes, ".//s:Description[@xml:lang='fr']", ns = ns))

# Create a data frame with the value, definition, and fr_definition columns
df <- data.frame(value, definition, fr_definition, stringsAsFactors = FALSE)

# Rename the value column to variable
df <- rename(df, variable = value)

df



```


# 25  Functions

## 25.1 Introduction

```{r}
library(tidyverse)
library(nycflights13)
```

## 25.2 Vector functions

### 25.2.1 Writing a function

-To turn this into a function you need three things:

-A name. Here we’ll use rescale01 because this function rescales a vector to lie between 0 and 1.

-The arguments. The arguments are things that vary across calls and our analysis above tells us that we have just one. We’ll call it x because this is the conventional name for a numeric vector.

-The body. The body is the code that’s repeated across all the calls.

### 25.2.2 Improving our function

-You might notice that the rescale01() function does some unnecessary work — instead of computing min() twice and max() once we could instead compute both the minimum and maximum in one step with range()

### 25.2.3 Mutate functions

-Now you’ve got the basic idea of functions, let’s take a look at a whole bunch of examples. We’ll start by looking at “mutate” functions, i.e. functions that work well inside of mutate() and filter() because they return an output of the same length as the input.

-Better see the book's demonstration.

### 25.2.4 Summary functions

-Another important family of vector functions is summary functions, functions that return a single value for use in summarize().

-Again, see the book to learn from the demonstration.

### 25.2.5 Exercises

1.Practice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need?

-lets turn.
```{r}
#mean(is.na(x))
#mean(is.na(y))
#mean(is.na(z))

#x / sum(x, na.rm = TRUE)
#y / sum(y, na.rm = TRUE)
#z / sum(z, na.rm = TRUE)

#round(x / sum(x, na.rm = TRUE) * 100, 1)
#round(y / sum(y, na.rm = TRUE) * 100, 1)
#round(z / sum(z, na.rm = TRUE) * 100, 1)

#
prop_na <- function(x) {
  mean(is.na(x))
}

normalize <- function(x) {
  x / sum(x, na.rm = TRUE)
}

percent <- function(x) {
  round(normalize(x) * 100, 1)
}
```

2. In the second variant of rescale01(), infinite values are left unchanged. Can you rewrite rescale01() so that -Inf is mapped to 0, and Inf is mapped to 1?
```{r}
rescale01 <- function(x) {
  rng <- range(x, na.rm = TRUE, finite = TRUE)
  ifelse(x == -Inf, 0, ifelse(x == Inf, 1, (x - rng[1]) / (rng[2] - rng[1])))
}
```

3. Given a vector of birthdates, write a function to compute the age in years.

```{r}
age <- function(x, today) {
  x <- as.Date(x)
  today <- as.Date(today)
  as.numeric(difftime(today, x, units = "weeks")) / 52.25
}
```

4. Write your own functions to compute the variance and skewness of a numeric vector. You can look up the definitions on Wikipedia or elsewhere.
```{r}
variance <- function(x) {
  n <- length(x)
  m <- mean(x)
  sum((x - m)^2) / (n - 1)
}

skewness <- function(x) {
  n <- length(x)
  m <- mean(x)
  s <- sqrt(variance(x))
  sum((x - m)^3) / (n * s^3)
}
```
5. Write both_na(), a summary function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors.
```{r}
both_na <- function(x, y) {
  sum(is.na(x) & is.na(y))
}
```
6. Read the documentation to figure out what the following functions do. Why are they useful even though they are so short?

```{r}
is_directory <- function(x) {
  file.info(x)$isdir
}
is_readable <- function(x) {
  file.access(x, 4) == 0
}
```

-Probably because the function of both verbs. is_directory is a function that simplifies the process of testing if a file is a directory and avoids typing the file.info function every time.

-is_readable also simplifies the process of testing if a file is readable and avoids typing the file.access function every time.


## 25.3 Data frame functions

### 25.3.1 Indirection and tidy evaluation

-When you start writing functions that use dplyr verbs you rapidly hit the problem of indirection. Let’s illustrate the problem with a very simple function: grouped_mean().

-Regardless of how we call grouped_mean() it always does df |> group_by(group_var) |> summarize(mean(mean_var)), instead of df |> group_by(group) |> summarize(mean(x)) or df |> group_by(group) |> summarize(mean(y)). This is a problem of indirection, and it arises because dplyr uses tidy evaluation to allow you to refer to the names of variables inside your data frame without any special treatment.

-Tidy evaluation includes a solution to this problem called embracing 🤗. Embracing a variable means to wrap it in braces so (e.g.) var becomes {{ var }}. Embracing a variable tells dplyr to use the value stored inside the argument, not the argument as the literal variable name. 

-One way to remember what’s happening is to think of {{ }} as looking down a tunnel — {{ var }} will make a dplyr function look inside of var rather than looking for a variable called var.

### 25.3.2 When to embrace?

-There are two terms to look for in the docs which correspond to the two most common sub-types of tidy evaluation:

-Data-masking: this is used in functions like arrange(), filter(), and summarize() that compute with variables.

-Tidy-selection: this is used for functions like select(), relocate(), and rename() that select variables.

### 25.3.3 Common use cases

-(Whenever you wrap summarize() in a helper, we think it’s good practice to set .groups = "drop" to both avoid the message and leave the data in an ungrouped state.)

-Furthermore, since the arguments to summarize are data-masking also means that the var argument to summary6() is data-masking. That means you can also summarize computed variables

-Another popular summarize() helper function is a version of count() that also computes proportions

-This function has three arguments: df, var, and sort, and only var needs to be embraced because it’s passed to count() which uses data-masking for all variables. Note that we use a default value for sort so that if the user doesn’t supply their own value it will default to FALSE.

-Here we embrace condition because it’s passed to filter() and var because it’s passed to distinct() and arrange().

We’ve made all these examples to take a data frame as the first argument, but if you’re working repeatedly with the same data, it can make sense to hardcode it. For example, the following function always works with the flights dataset and always selects time_hour, carrier, and flight since they form the compound primary key that allows you to identify a row.

### 25.3.4 Data-masking vs. tidy-selection

-Sometimes you want to select variables inside a function that uses data-masking. For example, imagine you want to write a count_missing() that counts the number of missing observations in rows.

-This doesn’t work because group_by() uses data-masking, not tidy-selection. We can work around that problem by using the handy pick() function, which allows you to use tidy-selection inside data-masking functions

-Another convenient use of pick() is to make a 2d table of counts. Here we count using all the variables in the rows and columns, then use pivot_wider() to rearrange the counts into a grid

### 25.3.5 Exercises

1.Using the datasets from nycflights13, write a function that:

  1.Finds all flights that were cancelled (i.e. is.na(arr_time)) or delayed by more than an hour.

flights |> filter_severe()

 2. Counts the number of cancelled flights and the number of flights delayed by more than an hour.

flights |> group_by(dest) |> summarize_severe()

 3. Finds all flights that were cancelled or delayed by more than a user supplied number of hours:

flights |> filter_severe(hours = 2)

 4. Summarizes the weather to compute the minimum, mean, and maximum, of a user supplied variable:

weather |> summarize_weather(temp)

 5. Converts the user supplied variable that uses clock time (e.g., dep_time, arr_time, etc.) into a decimal time (i.e. hours + (minutes / 60)).

flights |> standardize_time(sched_dep_time)

```{r}
library(nycflights13)

# Define a function to filter flights that were cancelled or delayed by more than an hour
filter_severe <- function(data, hours = 1) {
  if (!is.data.frame(data)) {
    stop("The data must be a data frame")
  }
  if (!all(c("dep_time", "dep_delay", "arr_time") %in% names(data))) {
    stop("The data must have the columns dep_time, dep_delay, and arr_time")
  }
  data %>%
    filter(is.na(arr_time) | dep_delay > hours * 60)
}

# Define a function to count the number of cancelled flights and the number of flights delayed by more than an hour by destination
summarize_severe <- function(data, hours = 1) {
  if (!is.data.frame(data)) {
    stop("The data must be a data frame")
  }
  if (!all(c("dep_time", "dep_delay", "arr_time", "dest") %in% names(data))) {
    stop("The data must have the columns dep_time, dep_delay, arr_time, and dest")
  }
  data %>%
    group_by(dest) %>%
    summarize(cancelled = sum(is.na(arr_time)),
              delayed = sum(dep_delay > hours * 60),
              .groups = "drop")
}

# Define a function to summarize the weather to compute the minimum, mean, and maximum of a user supplied variable
summarize_weather <- function(data, var) {
  if (!is.data.frame(data)) {
    stop("The data must be a data frame")
  }
  if (!is.character(var)) {
    stop("The var must be a character string")
  }
  if (!(var %in% names(data))) {
    stop("The var must be a column name of the data")
  }
  data %>%
    summarize(min = min(!!sym(var), na.rm = TRUE),
              mean = mean(!!sym(var), na.rm = TRUE),
              max = max(!!sym(var), na.rm = TRUE))
}

# Define a function to convert a variable that uses clock time into a decimal time
standardize_time <- function(data, var) {
  if (!is.data.frame(data)) {
    stop("The data must be a data frame")
  }
  if (!is.character(var)) {
    stop("The var must be a character string")
  }
  if (!(var %in% names(data))) {
    stop("The var must be a column name of the data")
  }
  data %>%
    mutate(!!var := floor(!!sym(var) / 100) + (!!sym(var) %% 100) / 60)
}
```

2. For each of the following functions list all arguments that use tidy evaluation and describe whether they use data-masking or tidy-selection: distinct(), count(), group_by(), rename_with(), slice_min(), slice_sample().

-The function distinct() uses tidy evaluation for its ... argument, which can be any number of variables or expressions. data-masking

-The function count() uses tidy evaluation for its ... and wt arguments; can be either variables or expressions. data-masking

-group_by() uses tidy evaluation for its ... argument, which can be any number of variables or expressions. data-masking 

-rename_with() uses tidy evaluation for its .cols and .fn arguments, which can be either variables or expressions.  tidy-selection

-slice_min() uses tidy evaluation for its ... and order_by arguments, which can be either variables or expressions. It uses data-masking.

-slice_sample() uses tidy evaluation for its ... and weight_by arguments. data-masking



3. Generalize the following function so that you can supply any number of variables to count.

```{r}
count_prop <- function(df, var, sort = FALSE) {
  df |>
    count({{ var }}, sort = sort) |>
    mutate(prop = n / sum(n))
}

#Lets do it. use the ... argument and the !!! operator to splice the variables into the count() function.

count_prop <- function(df, ..., sort = FALSE) {
  df |>
    count(!!!enquos(...), sort = sort) |>
    mutate(prop = n / sum(n))
}
```

## 25.4 Plot functions

### 25.4.1 More variables

-Please look the code examples in the book for better knowing what can be achieved.

### 25.4.2 Combining with other tidyverse

-Some of the most useful helpers combine a dash of data manipulation with ggplot2. For example, if you might want to do a vertical bar chart where you automatically sort the bars in frequency order using fct_infreq().

-We have to use a new operator here, := (commonly referred to as the “walrus operator”), because we are generating the variable name based on user-supplied data. Variable names go on the left hand side of =, but R’s syntax doesn’t allow anything to the left of = except for a single literal name. To work around this problem, we use the special operator := which tidy evaluation treats in exactly the same way as =.

-You can also get creative and display data summaries in other ways. You can find a cool application at https://gist.github.com/GShotwell/b19ef520b6d56f61a830fabb3454965b; it uses the axis labels to display the highest value. As you learn more about ggplot2, the power of your functions will continue to increase.

### 25.4.3 Labeling

-Wouldn’t it be nice if we could label the output with the variable and the bin width that was used? To do so, we’re going to have to go under the covers of tidy evaluation and use a function from the package we haven’t talked about yet: rlang. rlang is a low-level package that’s used by just about every other package in the tidyverse because it implements tidy evaluation (as well as many other useful tools).

-To solve the labeling problem we can use rlang::englue(). This works similarly to str_glue(), so any value wrapped in { } will be inserted into the string. But it also understands {{ }}, which automatically inserts the appropriate variable name.

### 25.4.4 Exercises

Build up a rich plotting function by incrementally implementing each of the steps below:

Draw a scatterplot given dataset and x and y variables.

Add a line of best fit (i.e. a linear model with no standard errors).

Add a title.

```{r}
plot_scatter <- function(data, x, y, title) {
  # Check if the data is a data frame
  if (!is.data.frame(data)) {
    stop("The data must be a data frame")
  }
  # Make sure the x and y are character strings
  if (!is.character(x) || !is.character(y)) {
    stop("The x and y must be character strings")
  }
  # Verify if the x and y are column names of the data
  if (!(x %in% names(data)) || !(y %in% names(data))) {
    stop("The x and y must be column names of the data")
  }
  # Identify if the title is a character string
  if (!is.character(title)) {
    stop("The title must be a character string")
  }
  # Plot the scatterplot of x and y
  plot(data[[x]], data[[y]], xlab = x, ylab = y)
  # Add a line of best fit using the lm function
  abline(lm(data[[y]] ~ data[[x]]))
  # And add a title using the title function
  title(main = title)
}

```

## 25.5 Style

-Generally, function names should be verbs, and arguments should be nouns. There are some exceptions: nouns are ok if the function computes a very well known noun (i.e. mean() is better than compute_mean()), or accessing some property of an object (i.e. coef() is better than get_coefficients()). 

-R also doesn’t care about how you use white space in your functions but future readers will. Continue to follow the rules from Chapter 4. Additionally, function() should always be followed by squiggly brackets ({}), and the contents should be indented by an additional two spaces. 

### 25.5.1 Exercises
1. Read the source code for each of the following two functions, puzzle out what they do, and then brainstorm better names.

f1 <- function(string, prefix) {
  str_sub(string, 1, str_length(prefix)) == prefix
}

f3 <- function(x, y) {
  rep(y, length.out = length(x))
}

-The function f1 takes a string and a prefix as arguments and returns TRUE if the string starts with the prefix and FALSE otherwise. A possible better name for this function is starts_with, which is more descriptive and consistent with the stringr package.

-f3 takes two vectors x and y as arguments and returns a vector of the same length as x that contains the values of y repeated as many times as necessary. A possible better name for this function is recycle, which is more suggestive of the recycling rule in R.

2. Take a function that you’ve written recently and spend 5 minutes brainstorming a better name for it and its arguments.

```{r}
#The age function just made in previous sections
age <- function(x, today) {
  x <- as.Date(x)
  today <- as.Date(today)
  as.numeric(difftime(today, x, units = "weeks")) / 52.25
}
```

-... Maybe I can improve this function by using more descriptive names for the arguments, such as birthdates.

3. Make a case for why norm_r(), norm_d() etc. would be better than rnorm(), dnorm(). Make a case for the opposite. How could you make the names even clearer?

-First one: 